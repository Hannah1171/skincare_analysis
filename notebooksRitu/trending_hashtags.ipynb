{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# STEP 1: Load your dataset (replace with actual path or query)\n",
    "df = pd.read_csv(\"/Users/ritushetkar/env_capstone/data/hashtags_posts.csv\", parse_dates=['createTimeISO'])\n",
    "\n",
    "\n",
    "\n",
    "# STEP 3: Aggregate to hashtag-day level\n",
    "df['post_date'] = df['createTimeISO'].dt.date\n",
    "\n",
    "daily_hashtags = (\n",
    "    df.groupby(['hashtag_name', 'post_date'])\n",
    "      .agg(\n",
    "          post_volume=('post_id', 'nunique'),\n",
    "          total_diggs=('diggCount', 'sum'),\n",
    "          total_shares=('shareCount', 'sum'),\n",
    "          total_comments=('commentCount', 'sum'),\n",
    "          total_plays=('playCount', 'sum'),\n",
    "          avg_duration=('video_duration', 'mean'),\n",
    "          avg_fans=('author_fans', 'mean')\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "daily_hashtags['post_date'] = pd.to_datetime(daily_hashtags['post_date'])\n",
    "daily_hashtags = daily_hashtags.sort_values(['hashtag_name', 'post_date'])\n",
    "\n",
    "for col in ['post_volume', 'total_diggs', 'total_shares', 'total_comments', 'total_plays']:\n",
    "    daily_hashtags[f'{col}_7d'] = (\n",
    "        daily_hashtags.groupby('hashtag_name')[col]\n",
    "        .transform(lambda x: x.rolling(window=7, min_periods=1).sum())\n",
    "    )\n",
    "\n",
    "# STEP 5: Add change rates\n",
    "daily_hashtags['volume_change_7d'] = (\n",
    "    daily_hashtags.groupby('hashtag_name')['post_volume_7d']\n",
    "    .pct_change().replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    ")\n",
    "\n",
    "# STEP 6: Label as trending (if volume jumped >50% day-over-day)\n",
    "daily_hashtags['is_trending'] = (daily_hashtags['volume_change_7d'] > 0.5).astype(int)\n",
    "\n",
    "# STEP 7: Fill missing values\n",
    "daily_hashtags.fillna(0, inplace=True)\n",
    "\n",
    "# FINAL FEATURE SET\n",
    "features = [\n",
    "    'post_volume_7d', 'total_diggs_7d', 'total_shares_7d', \n",
    "    'total_comments_7d', 'total_plays_7d', 'avg_duration', 'avg_fans', 'volume_change_7d'\n",
    "]\n",
    "\n",
    "X = daily_hashtags[features]\n",
    "y = daily_hashtags['is_trending']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'post_date' is a datetime object (in case it's a string)\n",
    "df['post_date'] = pd.to_datetime(df['post_date'])\n",
    "\n",
    "# Get unique dates and sort them descending\n",
    "unique_dates = df['post_date'].dropna().drop_duplicates().sort_values(ascending=True)\n",
    "\n",
    "print(unique_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_hashtags[daily_hashtags['is_trending']==1].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Get predicted probabilities for the positive class (trending = 1)\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Create a DataFrame with predictions\n",
    "pred_df = daily_hashtags.loc[X_test.index].copy()\n",
    "pred_df['trending_probability'] = probs\n",
    "\n",
    "# 3. Filter for today's hashtags (or latest date)\n",
    "latest_date = pred_df['post_date'].max()\n",
    "today_hashtags = pred_df[pred_df['post_date'] == latest_date]\n",
    "\n",
    "# 4. Get top 10 most likely trending hashtags\n",
    "top_trending = (\n",
    "    today_hashtags[['hashtag_name', 'trending_probability']]\n",
    "    .sort_values(by='trending_probability', ascending=False)\n",
    "    .drop_duplicates('hashtag_name')\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "top_trending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Get predicted probabilities for the positive class (trending = 1)\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Create a DataFrame with predictions\n",
    "pred_df = daily_hashtags.loc[X_test.index].copy()\n",
    "pred_df['trending_probability'] = probs\n",
    "\n",
    "# 3. Filter for today's hashtags (or latest date)\n",
    "latest_date = pred_df['post_date'].max()\n",
    "today_hashtags = pred_df[pred_df['post_date'] == latest_date]\n",
    "\n",
    "# 4. Get top 20 most likely trending hashtags\n",
    "top_trending = (\n",
    "    today_hashtags[['hashtag_name', 'trending_probability']]\n",
    "    .sort_values(by='trending_probability', ascending=False)\n",
    "    .drop_duplicates('hashtag_name')\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "# 5. Show the result\n",
    "print(top_trending)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Get predicted probabilities for the positive class (trending = 1)\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Create a DataFrame with predictions\n",
    "pred_df = daily_hashtags.loc[X_test.index].copy()\n",
    "pred_df['trending_probability'] = probs\n",
    "\n",
    "# 3. Filter for hashtags from the last 7 days\n",
    "latest_date = pred_df['post_date'].max()\n",
    "start_date = latest_date - pd.Timedelta(days=6)  # last 7 days including today\n",
    "\n",
    "week_hashtags = pred_df[(pred_df['post_date'] >= start_date) & (pred_df['post_date'] <= latest_date)]\n",
    "\n",
    "# 4. Get top 20 most likely trending hashtags (across all those days)\n",
    "top_trending_week = (\n",
    "    week_hashtags[['hashtag_name', 'post_date', 'trending_probability']]\n",
    "    .sort_values(by='trending_probability', ascending=False)\n",
    "    .drop_duplicates('hashtag_name')  # ensures each hashtag appears only once\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "# 5. Show the result\n",
    "print(top_trending_week)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Get predicted probabilities for the positive class (trending = 1)\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Create a DataFrame with predictions\n",
    "pred_df = daily_hashtags.loc[X_test.index].copy()\n",
    "pred_df['trending_probability'] = probs\n",
    "\n",
    "# 3. Filter for hashtags from the last 30 days\n",
    "latest_date = pred_df['post_date'].max()\n",
    "start_date_month = latest_date - pd.Timedelta(days=29)  # last 30 days including today\n",
    "\n",
    "month_hashtags = pred_df[\n",
    "    (pred_df['post_date'] >= start_date_month) & (pred_df['post_date'] <= latest_date)\n",
    "]\n",
    "\n",
    "# 4. Get top 20 most likely trending hashtags across the month\n",
    "top_trending_month = (\n",
    "    month_hashtags[['hashtag_name', 'post_date', 'trending_probability']]\n",
    "    .sort_values(by='trending_probability', ascending=False)\n",
    "    .drop_duplicates('hashtag_name')  # only one row per hashtag\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "# 5. Show the result\n",
    "print(top_trending_month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# STEP 1: Predict trending probabilities\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "pred_df = daily_hashtags.loc[X_test.index].copy()\n",
    "pred_df['trending_probability'] = probs\n",
    "\n",
    "# STEP 2: Define date ranges\n",
    "latest_date = pred_df['post_date'].max()\n",
    "last_7_days = latest_date - pd.Timedelta(days=6)\n",
    "last_30_days = latest_date - pd.Timedelta(days=29)\n",
    "\n",
    "# STEP 3: Get Top N for Weekly\n",
    "top_week = (\n",
    "    pred_df[pred_df['post_date'] >= last_7_days]\n",
    "    .sort_values(by='trending_probability', ascending=False)\n",
    "    .drop_duplicates('hashtag_name')\n",
    "    .head(20)\n",
    "    .assign(source='weekly')\n",
    ")\n",
    "\n",
    "# STEP 4: Get Top N for Monthly\n",
    "top_month = (\n",
    "    pred_df[pred_df['post_date'] >= last_30_days]\n",
    "    .sort_values(by='trending_probability', ascending=False)\n",
    "    .drop_duplicates('hashtag_name')\n",
    "    .head(20)\n",
    "    .assign(source='monthly')\n",
    ")\n",
    "\n",
    "# STEP 5: Merge to compare\n",
    "combined = pd.concat([top_week, top_month])\n",
    "combined_summary = (\n",
    "    combined.groupby('hashtag_name')\n",
    "    .agg(\n",
    "        sources=('source', lambda x: ', '.join(sorted(set(x)))),\n",
    "        max_probability=('trending_probability', 'max'),\n",
    "        most_recent_date=('post_date', 'max')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by='max_probability', ascending=False)\n",
    ")\n",
    "\n",
    "print(combined_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_combined = combined_summary.head(20)  # visualize top 20 from comparison\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_combined['hashtag_name'], top_combined['max_probability'], color='skyblue')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Max Trending Probability')\n",
    "plt.title('Top Hashtags (Weekly vs Monthly Trending Overlap)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HASHTAGS with XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# STEP 1: Load dataset\n",
    "df = pd.read_csv(\"/Users/ritushetkar/env_capstone/data/hashtags_posts.csv\", parse_dates=['createTimeISO'])\n",
    "\n",
    "# STEP 2: Aggregate to hashtag-day level\n",
    "df['post_date'] = df['createTimeISO'].dt.date\n",
    "\n",
    "daily_hashtags = (\n",
    "    df.groupby(['hashtag_name', 'post_date'])\n",
    "      .agg(\n",
    "          post_volume=('post_id', 'nunique'),\n",
    "          total_diggs=('diggCount', 'sum'),\n",
    "          total_shares=('shareCount', 'sum'),\n",
    "          total_comments=('commentCount', 'sum'),\n",
    "          total_plays=('playCount', 'sum'),\n",
    "          avg_duration=('video_duration', 'mean'),\n",
    "          avg_fans=('author_fans', 'mean')\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# STEP 3: Rolling features (7-day window)\n",
    "daily_hashtags['post_date'] = pd.to_datetime(daily_hashtags['post_date'])\n",
    "daily_hashtags = daily_hashtags.sort_values(['hashtag_name', 'post_date'])\n",
    "\n",
    "for col in ['post_volume', 'total_diggs', 'total_shares', 'total_comments', 'total_plays']:\n",
    "    daily_hashtags[f'{col}_7d'] = (\n",
    "        daily_hashtags.groupby('hashtag_name')[col]\n",
    "        .transform(lambda x: x.rolling(window=7, min_periods=1).sum())\n",
    "    )\n",
    "\n",
    "# STEP 4: Growth signal\n",
    "daily_hashtags['volume_change_7d'] = (\n",
    "    daily_hashtags.groupby('hashtag_name')['post_volume_7d']\n",
    "    .pct_change().replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    ")\n",
    "\n",
    "# STEP 5: Label trending (volume surge > 50%)\n",
    "daily_hashtags['is_trending'] = (daily_hashtags['volume_change_7d'] > 0.5).astype(int)\n",
    "\n",
    "# STEP 6: Prepare features\n",
    "daily_hashtags.fillna(0, inplace=True)\n",
    "\n",
    "#features = [\n",
    " #   'post_volume_7d', 'total_diggs_7d', 'total_shares_7d',\n",
    "  #  'total_comments_7d', 'total_plays_7d', 'avg_duration', 'avg_fans', 'volume_change_7d'\n",
    "#]\n",
    "\n",
    "features = [\n",
    "    'post_volume_7d', 'total_diggs_7d', 'total_shares_7d',\n",
    "    'total_comments_7d', 'total_plays_7d', 'avg_duration', 'avg_fans'\n",
    "]\n",
    "\n",
    "X = daily_hashtags[features]\n",
    "y = daily_hashtags['is_trending']\n",
    "\n",
    "# STEP 7: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# STEP 8: Model training\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# STEP 10: SHAP for interpretability\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7399\n",
      "           1       0.97      0.91      0.94       937\n",
      "\n",
      "    accuracy                           0.99      8336\n",
      "   macro avg       0.98      0.96      0.97      8336\n",
      "weighted avg       0.99      0.99      0.99      8336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 9: Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 11: Predict trending probabilities\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "pred_df = daily_hashtags.loc[X_test.index].copy()\n",
    "pred_df['trending_probability'] = probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Top Trending Hashtags Today (with at least 5 posts in the last 7 days)\n",
      "            hashtag_name  post_volume_7d  trending_probability\n",
      "34382  skincarethatworks            11.0              0.000626\n",
      "33017           skincare           181.0              0.000098\n",
      "21213     koreanskincare            44.0              0.000033\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# STEP 12: Top trending for the latest date with minimum post filter\n",
    "MIN_POSTS = 10 # set your minimum post threshold here\n",
    "\n",
    "latest_date = pred_df['post_date'].max()\n",
    "today_trends = (\n",
    "    pred_df[\n",
    "        (pred_df['post_date'] == latest_date) &\n",
    "        (pred_df['post_volume_7d'] >= MIN_POSTS)\n",
    "    ]\n",
    "    .sort_values(by='trending_probability', ascending=False)\n",
    "    .drop_duplicates('hashtag_name')\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "print(\"🔥 Top Trending Hashtags Today (with at least 5 posts in the last 7 days)\")\n",
    "print(today_trends[['hashtag_name', 'post_volume_7d', 'trending_probability']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "with MIN Posts as 0\n",
    "🔥 Top Trending Hashtags Today (with at least 5 posts in the last 7 days)\n",
    "                hashtag_name  post_volume_7d  trending_probability\n",
    "30249           ringanafresh             2.0              0.962396\n",
    "12775                frische             5.0              0.044179\n",
    "4676             beautyhacks             8.0              0.011727\n",
    "32144  sensitiveskinfriendly             6.0              0.005687\n",
    "17617              hautliebe             8.0              0.003768\n",
    "24504                momlife             7.0              0.003364\n",
    "5174             beautytipps             9.0              0.002100\n",
    "34049         skincarereview             8.0              0.001337\n",
    "34382      skincarethatworks            11.0              0.000626\n",
    "33017               skincare           181.0              0.000098\n",
    "21213         koreanskincare            44.0              0.000033\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "with MIN Posts as 5\n",
    "Top Trending Hashtags Today (with at least 5 posts in the last 7 days)\n",
    "                hashtag_name  post_volume_7d  trending_probability\n",
    "12775                frische             5.0              0.044179\n",
    "4676             beautyhacks             8.0              0.011727\n",
    "32144  sensitiveskinfriendly             6.0              0.005687\n",
    "17617              hautliebe             8.0              0.003768\n",
    "24504                momlife             7.0              0.003364\n",
    "5174             beautytipps             9.0              0.002100\n",
    "34049         skincarereview             8.0              0.001337\n",
    "34382      skincarethatworks            11.0              0.000626\n",
    "33017               skincare           181.0              0.000098\n",
    "21213         koreanskincare            44.0              0.000033\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "with 10 min posts\n",
    " Top Trending Hashtags Today (with at least 5 posts in the last 7 days)\n",
    "            hashtag_name  post_volume_7d  trending_probability\n",
    "34382  skincarethatworks            11.0              0.000626\n",
    "33017           skincare           181.0              0.000098\n",
    "21213     koreanskincare            44.0              0.000033\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    " with 20 min posts\n",
    "Top Trending Hashtags Today (with at least 5 posts in the last 7 days)\n",
    "         hashtag_name  post_volume_7d  trending_probability\n",
    "33017        skincare           181.0              0.000098\n",
    "21213  koreanskincare            44.0              0.000033\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "with 50 min posts \n",
    "Top Trending Hashtags Today (with at least 5 posts in the last 7 days)\n",
    "      hashtag_name  post_volume_7d  trending_probability\n",
    "33017     skincare           181.0              0.000098\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the most recent date in the prediction set\n",
    "latest_date = pred_df['post_date'].max()\n",
    "week_start = latest_date - pd.Timedelta(days=6)\n",
    "month_start = latest_date - pd.Timedelta(days=29)\n",
    "\n",
    "# WEEKLY (Step 13)\n",
    "week_trends = (\n",
    "    pred_df[\n",
    "        (pred_df['post_date'] >= week_start) &\n",
    "        (pred_df['post_volume_7d'] >= MIN_POSTS)\n",
    "    ]\n",
    "    .sort_values(by='trending_probability', ascending=False)\n",
    "    .drop_duplicates('hashtag_name')\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "# MONTHLY (Step 14)\n",
    "month_trends = (\n",
    "    pred_df[\n",
    "        (pred_df['post_date'] >= month_start) &\n",
    "        (pred_df['post_volume_7d'] >= MIN_POSTS)\n",
    "    ]\n",
    "    .sort_values(by='trending_probability', ascending=False)\n",
    "    .drop_duplicates('hashtag_name')\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "# STEP 15: Weekly vs Monthly comparison\n",
    "week_trends['source'] = 'weekly'\n",
    "month_trends['source'] = 'monthly'\n",
    "comparison = pd.concat([week_trends, month_trends])\n",
    "\n",
    "comparison_summary = (\n",
    "    comparison.groupby('hashtag_name')\n",
    "    .agg(\n",
    "        sources=('source', lambda x: ', '.join(sorted(set(x)))),\n",
    "        max_probability=('trending_probability', 'max'),\n",
    "        most_recent_date=('post_date', 'max')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by='max_probability', ascending=False)\n",
    ")\n",
    "\n",
    "print(\"📊 Weekly vs Monthly Trending Comparison:\")\n",
    "print(comparison_summary[comparison_summary['max_probability']>0.4].head(20))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
