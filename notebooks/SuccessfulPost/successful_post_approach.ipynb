{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85975cd",
   "metadata": {},
   "source": [
    "#### Post viralness (views) drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import spacy\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv(\"../data/filtered_data/comments_posts_transcripts.csv\", encoding='utf-8')\n",
    "\n",
    "df['createTimeISO'] = (\n",
    "    pd.to_datetime(df['createTimeISO'], utc=True, errors='coerce')\n",
    "      .dt.tz_convert('Europe/Berlin')\n",
    ")\n",
    "\n",
    "nine_months = pd.Timestamp.utcnow() - pd.DateOffset(months=9)\n",
    "df = df[df['createTimeISO'] >= nine_months]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989ebe1",
   "metadata": {},
   "source": [
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199cc129",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['textLanguage'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e664878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(row, include_description=True):\n",
    "    # Start with caption\n",
    "    texts = []\n",
    "    if pd.notna(row['text']):\n",
    "        texts.append(str(row['text']))\n",
    "    if pd.notna(row['transcribed_text']):\n",
    "        texts.append(str(row['transcribed_text']))\n",
    "    if include_description and pd.notna(row['video_description']):\n",
    "        texts.append(str(row['video_description']))\n",
    "    return ' '.join(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the concatenated text column FIRST\n",
    "text_cols = ['text', 'video_description', 'transcribed_text']\n",
    "df['text_all'] = df[text_cols].fillna('').agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f158d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All three fields\n",
    "#df['processed_text'] = df.apply(lambda row: combine_text(row, include_description=True), axis=1)\n",
    "\n",
    "# Or, only caption + transcript\n",
    "df['combined_text'] = df.apply(lambda row: combine_text(row, include_description=False), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd14447",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # English small model\n",
    "\n",
    "def clean_text(text, stopwords=None, lemmatize=False):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\w+\", \" \", text)\n",
    "    text = emoji.replace_emoji(text, replace=lambda e, _: f\" {e} \")\n",
    "    text = re.sub(r\"[^a-zA-ZäöüÄÖÜß0-9\\s#?!\\U0001F600-\\U0001F64F]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Use spaCy tokenizer for more robust tokenization\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    tokens = [re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", t) for t in tokens]\n",
    "\n",
    "    if stopwords:\n",
    "        tokens = [t for t in tokens if t not in stopwords and len(t) > 2]\n",
    "    else:\n",
    "        tokens = [t for t in tokens if len(t) > 2]\n",
    "\n",
    "    if lemmatize and tokens:\n",
    "        doc = nlp(\" \".join(tokens))\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NLTK_DATA\"] = \"/Users/hannahschlaucher/nltk_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95777675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.data.path.append('/Users/hannahschlaucher/nltk_data')\n",
    "nltk.data.path.append('/Users/hannahschlaucher/nltk_data')\n",
    "nltk.data.path.append('/Users/hannahschlaucher/nltk_data')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pos_enhanced_preprocessing(text):\n",
    "    meaningful_pos = {'NN', 'NNS', 'NNP', 'NNPS',\n",
    "                      'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\n",
    "                      'JJ', 'JJR', 'JJS',\n",
    "                      'RB', 'RBR', 'RBS'}\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    meaningful_tokens = [token for token, pos in pos_tags if pos in meaningful_pos]\n",
    "    return ' '.join(meaningful_tokens)\n",
    "\n",
    "#df['combined_text_processed'] = df['combined_text_processed'].apply(pos_enhanced_preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db767af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords = STOP_WORDS\n",
    "\n",
    "df['combined_text_processed'] = df['combined_text'].apply(lambda x: clean_text(x, stopwords=stopwords, lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ba856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"processed_combined_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b220b71",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time based categories\n",
    "\n",
    "# Hour of posting (0–23)\n",
    "df['hour_posting'] = df['createTimeISO'].dt.hour\n",
    "\n",
    "# Day of week (0=Monday, …, 6=Sunday)\n",
    "df['day_of_week'] = df['createTimeISO'].dt.dayofweek\n",
    "\n",
    "# Is weekend flag (True if Saturday or Sunday)\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6])\n",
    "\n",
    "# Time period categories\n",
    "def categorize_period(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "df['time_period'] = df['hour_posting'].apply(categorize_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TDIDF \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=8000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    stop_words='english',  # or your own stopword list\n",
    "    lowercase=False,       # Already lowercased during preprocessing\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\b\\w+\\b'\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(df['combined_text_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd37c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_statistics(text):\n",
    "    \"\"\"\n",
    "    Extract statistical features from text content\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return pd.Series([0, 0, 0, 0, 0])\n",
    "    \n",
    "    # Basic statistics\n",
    "    word_count = len(str(text).split())\n",
    "    sentence_count = len(str(text).split('.'))\n",
    "    \n",
    "    # Advanced statistics\n",
    "    avg_word_length = np.mean([len(word) for word in str(text).split()]) if word_count > 0 else 0\n",
    "    exclamation_count = str(text).count('!')\n",
    "    question_count = str(text).count('?')\n",
    "    \n",
    "    return pd.Series([word_count, sentence_count, \n",
    "                     avg_word_length, exclamation_count, question_count])\n",
    "\n",
    "\n",
    "# Extract text statistics for all text fields\n",
    "text_stats_columns = ['word_count', 'sentence_count', \n",
    "                     'avg_word_length', 'exclamation_count', 'question_count']\n",
    "\n",
    "df[text_stats_columns] = df['combined_text_processed'].apply(extract_text_statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    \"\"\"Extract hashtags from text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    hashtags = re.findall(r'#\\w+', text.lower())\n",
    "    return [tag.replace('#', '') for tag in hashtags]\n",
    "\n",
    "# Extract hashtags\n",
    "df['hashtag_list'] = df['text'].apply(extract_hashtags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# 1. Flatten all hashtags and count them\n",
    "all_hashtags = [tag for tags in df['hashtag_list'].dropna() for tag in (tags if isinstance(tags, list) else [])]\n",
    "hashtag_counts = Counter(all_hashtags)\n",
    "\n",
    "# 2. Calculate the 40th percentile threshold (so we keep hashtags used more often than 40% of the others)\n",
    "counts = np.array(list(hashtag_counts.values()))\n",
    "threshold = np.percentile(counts, 99)  # \"above 40th percentile\" == 60th percentile and up\n",
    "\n",
    "# 3. Build the set of most-used hashtags\n",
    "popular_hashtags = {tag for tag, count in hashtag_counts.items() if count >= threshold}\n",
    "\n",
    "# 4. Keep only those hashtags for each post\n",
    "def filter_popular(tags):\n",
    "    if not isinstance(tags, list):\n",
    "        return []\n",
    "    return [tag for tag in tags if tag in popular_hashtags]\n",
    "\n",
    "df['popular_hashtags'] = df['hashtag_list'].apply(filter_popular)\n",
    "\n",
    "# 5. One-hot encode\n",
    "mlb = MultiLabelBinarizer()\n",
    "hashtag_features = mlb.fit_transform(df['popular_hashtags'])\n",
    "hashtag_feature_names = ['hashtag_' + h for h in mlb.classes_]\n",
    "df_hashtag_features = pd.DataFrame(hashtag_features, columns=hashtag_feature_names, index=df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a555b",
   "metadata": {},
   "source": [
    "### Define target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb94729",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1   = df[\"playCount\"].quantile(0.25)\n",
    "Q3   = df[\"playCount\"].quantile(0.75)\n",
    "IQR  = Q3 - Q1\n",
    "\n",
    "lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "mask  = df[\"playCount\"].between(lower, upper)\n",
    "\n",
    "df_clean = df.loc[mask].copy().reset_index(drop=True)\n",
    "\n",
    "# views_log normalises heavy-tailed reach\n",
    "df['viral_score']  = np.log1p(df['playCount'])\n",
    "\n",
    "threshold = df['viral_score'].quantile(0.75)\n",
    "df['is_viral'] = (df['viral_score'] > threshold).astype(int)\n",
    "\n",
    "y = df['viral_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5eee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# select only numeric cols\n",
    "num_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# full correlation matrix\n",
    "corr_matrix = num_df.corr()\n",
    "#print(corr_matrix)\n",
    "\n",
    "# list high-corr pairs\n",
    "upper = corr_matrix.abs().where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr = (upper.stack()\n",
    "                .loc[lambda x: x > 0.7]\n",
    "                .sort_values(ascending=False))\n",
    "print(high_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5a4307",
   "metadata": {},
   "source": [
    "### Combine all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "additional_cols = [\n",
    "    'isAd',\n",
    "    'author_fans',\n",
    "    'video_duration',\n",
    "    'isSponsored',\n",
    "    'hour_posting', 'day_of_week', 'is_weekend', 'time_period',\n",
    "    'word_count', 'sentence_count',\n",
    "    'avg_word_length', 'exclamation_count', 'question_count'\n",
    "]\n",
    "\n",
    "# 1. Drop rows with NaNs in additional features\n",
    "df_clean = df.dropna(subset=additional_cols)\n",
    "\n",
    "# 2. Reset index so everything lines up in order\n",
    "df_clean = df_clean.reset_index(drop=True)\n",
    "df_hashtag_features_clean = df_hashtag_features.iloc[df_clean.index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cef4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop rows with NaNs in additional features\n",
    "df_clean = df.dropna(subset=additional_cols).reset_index(drop=True)\n",
    "df_hashtag_features_clean = df_hashtag_features.iloc[df_clean.index].reset_index(drop=True)\n",
    "\n",
    "# 2. Drop outliers from df_clean\n",
    "low, high = 0.0, 1\n",
    "num_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "mask = np.ones(len(df_clean), dtype=bool)\n",
    "\n",
    "for col in num_cols:\n",
    "    p1, p99 = df_clean[col].quantile([low, high])\n",
    "    mask &= df_clean[col].between(p1, p99)\n",
    "\n",
    "# Get remaining indices before reset\n",
    "remaining_idx = df_clean[mask].index\n",
    "\n",
    "# Apply mask to both DataFrames\n",
    "df_clean = df_clean.loc[remaining_idx].reset_index(drop=True)\n",
    "df_hashtag_features_clean = df_hashtag_features_clean.loc[remaining_idx].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Concatenate hashtag features to cleaned DataFrame (indices now match)\n",
    "df_clean = pd.concat([df_clean, df_hashtag_features_clean], axis=1)\n",
    "\n",
    "# 4. Convert booleans to int\n",
    "for col in ['isAd', 'isSponsored', 'is_weekend']:\n",
    "    df_clean[col] = df_clean[col].astype(int)\n",
    "\n",
    "# 5. One-hot encode 'time_period'\n",
    "df_clean = pd.get_dummies(df_clean, columns=['time_period'], drop_first=True)\n",
    "\n",
    "# 6. Prepare final list of feature columns (including hashtag columns)\n",
    "hashtag_cols = list(df_hashtag_features_clean.columns)\n",
    "additional_cols_final = (\n",
    "    [col for col in additional_cols if col != 'time_period'] +\n",
    "    [c for c in df_clean.columns if c.startswith('time_period_')] +\n",
    "    hashtag_cols\n",
    ")\n",
    "\n",
    "# 7. Extract and standardize features\n",
    "additional_data = df_clean[additional_cols_final]\n",
    "scaler = StandardScaler()\n",
    "additional_data_scaled = scaler.fit_transform(additional_data)\n",
    "additional_sparse = csr_matrix(additional_data_scaled)\n",
    "\n",
    "# 8. Make sure your TF-IDF matrix is in the same row order as df_clean\n",
    "X_tfidf_clean = X_tfidf[df_clean.index]\n",
    "\n",
    "\n",
    "# 9. Combine all features\n",
    "X_combined = hstack([X_tfidf_clean, additional_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b6436",
   "metadata": {},
   "source": [
    "### Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0da3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TF-IDF shape:\", X_tfidf_clean.shape)\n",
    "print(\"Additional sparse shape:\", additional_sparse.shape)\n",
    "print(\"Hashtag features shape:\", df_hashtag_features.shape)\n",
    "print(\"X_combined shape:\", X_combined.shape)\n",
    "print(\"y length:\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5707721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data sorted by createTimeISO before this step\n",
    "n_total = X_combined.shape[0]\n",
    "split_point = int(n_total * 0.8)\n",
    "\n",
    "train_idx = np.arange(0, split_point)\n",
    "test_idx = np.arange(split_point, n_total)\n",
    "\n",
    "X_train = X_combined[train_idx]\n",
    "X_test = X_combined[test_idx]\n",
    "y_train = y.iloc[train_idx]\n",
    "y_test = y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0873a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    X_train_cv, X_val_cv = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_cv, y_val_cv = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    xgb_cv = XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        tree_method='hist',\n",
    "        eval_metric='rmse',\n",
    "        early_stopping_rounds=20\n",
    "    )\n",
    "\n",
    "    xgb_cv.fit(\n",
    "        X_train_cv, y_train_cv,\n",
    "        eval_set=[(X_val_cv, y_val_cv)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    y_val_pred = xgb_cv.predict(X_val_cv)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_cv, y_val_pred))\n",
    "    r2 = r2_score(y_val_cv, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_cv, y_val_pred)\n",
    "\n",
    "    rmse_scores.append(rmse)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "print(\"Temporal CV RMSE (mean):\", np.mean(rmse_scores))\n",
    "print(\"Temporal CV R² (mean):\", np.mean(r2_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_val_pred, y_val_cv, alpha=0.5)\n",
    "plt.plot([y_val_cv.min(), y_val_cv.max()],\n",
    "         [y_val_cv.min(), y_val_cv.max()],\n",
    "         'r--')\n",
    "plt.xlabel(\"Predicted log(views)\")\n",
    "plt.ylabel(\"Actual log(views)\")\n",
    "plt.title(\"Predicted vs. Actual (Log Scale)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_mean = y_train.mean()\n",
    "baseline_rmse = np.sqrt(np.mean((y_test - y_mean)**2))\n",
    "print(\"Baseline RMSE:\", baseline_rmse)\n",
    "print(\"y_train std:\", y_train.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da7523",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc70fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Grab the scaled block (n_samples × 86)\n",
    "X_scaled = additional_data_scaled  \n",
    "\n",
    "#Recover the real-unit values for all samples\n",
    "means  = scaler.mean_    # shape (86,)\n",
    "scales = scaler.scale_   # shape (86,)\n",
    "X_orig = X_scaled * scales + means  # still (n_samples, 86)\n",
    "\n",
    "# Wrap in a DataFrame for easy lookup\n",
    "df_orig = pd.DataFrame(\n",
    "    X_orig,\n",
    "    columns=additional_cols_final,\n",
    "    index=df_clean.index\n",
    ")\n",
    "\n",
    "# Slice out the train vs. test rows\n",
    "df_train_orig = df_orig.loc[train_idx]  \n",
    "df_test_orig  = df_orig.loc[test_idx]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc02bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Length of train_idx:\", len(train_idx))\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Length of test_idx:\", len(test_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973cfefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Combine feature names\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "feature_names = tfidf_feature_names + additional_cols_final\n",
    "\n",
    "\n",
    "# Convert your sparse train/test matrices into DataFrames\n",
    "X_train_df = pd.DataFrame(\n",
    "    X_train.toarray(),\n",
    "    columns=feature_names,\n",
    "    index=train_idx\n",
    ")\n",
    "\n",
    "X_test_df = pd.DataFrame(\n",
    "    X_test.toarray(),\n",
    "    columns=feature_names,\n",
    "    index=test_idx\n",
    ")\n",
    "\n",
    "# SHAP explainer\n",
    "explainer = shap.TreeExplainer(xgb_cv, X_train_df)  # your trained XGBoost model\n",
    "shap_values = explainer.shap_values(X_test_df)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test_df, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean absolute SHAP value per feature\n",
    "shap_values = explainer(X_test_df)\n",
    "shap.plots.bar(shap_values, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. build the full importance DataFrame\n",
    "df_imp = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"mean_abs_shap\": np.abs(shap_values.values).mean(axis=0)\n",
    "    })\n",
    "    .sort_values(\"mean_abs_shap\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 2. filter by threshold (10% of max)\n",
    "thresh    = 0.1 * df_imp[\"mean_abs_shap\"].max()\n",
    "relevant  = df_imp[df_imp[\"mean_abs_shap\"] >= thresh]\n",
    "\n",
    "relevant_features = pd.DataFrame(relevant)\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a9066",
   "metadata": {},
   "source": [
    "### Now lets look into the actual distribution, how the Shap values changes for each value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87721f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(xgb_cv, X_train_df)\n",
    "shap_values = explainer(X_test_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_features = [\"author_fans\", \"video_duration\", \"word_count\", \"isAd\", \"hour_posting\"]\n",
    "\n",
    "for feature in top_features:\n",
    "    #values = df_test_orig.loc[test_idx, feature].reset_index(drop=True)\n",
    "    values = df_test_orig[feature].reset_index(drop=True)\n",
    "    shap_vals = shap_values[:, feature].values.ravel()\n",
    "    df_plot = pd.DataFrame({feature: values, \"shap_value\": shap_vals})\n",
    "\n",
    "    # 1. Binary feature\n",
    "    if set(df_plot[feature].dropna().unique()) <= {0, 1}:\n",
    "        summary = df_plot.groupby(feature)[\"shap_value\"].mean().reset_index()\n",
    "        summary.rename(columns={\"shap_value\": \"mean_shap_value\"}, inplace=True)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.bar(summary[feature].astype(str), summary[\"mean_shap_value\"])\n",
    "\n",
    "    # 2. Discrete feature like 'hour'\n",
    "    elif feature == \"hour_posting\":\n",
    "        summary = df_plot.groupby(feature)[\"shap_value\"].mean().reset_index()\n",
    "        summary.rename(columns={\"shap_value\": \"mean_shap_value\"}, inplace=True)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(summary[feature], summary[\"mean_shap_value\"], marker=\"o\")\n",
    "\n",
    "    # 3. Continuous feature\n",
    "    else:\n",
    "        p5, p95 = np.percentile(values, [5, 95])\n",
    "        df_plot = df_plot[(values >= p5) & (values <= p95)]\n",
    "\n",
    "        bins = np.linspace(df_plot[feature].min(), df_plot[feature].max(), 20)\n",
    "        df_plot[\"bin\"] = pd.cut(df_plot[feature], bins=bins, include_lowest=True)\n",
    "\n",
    "        summary = df_plot.groupby(\"bin\").agg(\n",
    "            mean_shap_value=(\"shap_value\", \"mean\"),\n",
    "            bin_center=(feature, \"mean\")\n",
    "        ).dropna().reset_index()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(summary[\"bin_center\"], summary[\"mean_shap_value\"], marker=\"o\")\n",
    "\n",
    "    # Save and show\n",
    "    summary.to_csv(f\"shap_vs_{feature}.csv\", index=False)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Mean SHAP Value\")\n",
    "    plt.title(f\"SHAP vs. {feature}\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7843f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_cv.__class__)               # Should be XGBRegressor\n",
    "print(xgb_cv.n_outputs_ if hasattr(xgb_cv, 'n_outputs_') else \"unknown\")\n",
    "print(shap_values.values.shape)      # Check for (731,) vs (731, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbade1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_features = [\"author_fans\", \"video_duration\", \"word_count\", \"isAd\",'hour_posting']\n",
    "\n",
    "for feature in top_features:\n",
    "    # Get raw feature values from test set aligned with SHAP values\n",
    "    feature_values = df_test_orig.loc[test_idx, feature].reset_index(drop=True)\n",
    "    shap_val = shap_values[:, feature].values\n",
    "\n",
    "    if feature == \"isAd\":\n",
    "        # Binary feature → group by 0/1\n",
    "        df_plot = pd.DataFrame({\n",
    "            feature: feature_values,\n",
    "            \"shap_value\": shap_val\n",
    "        })\n",
    "\n",
    "        group_means = df_plot.groupby(feature)[\"shap_value\"].mean()\n",
    "        output_df = group_means.reset_index().rename(columns={\"shap_value\": \"mean_shap_value\"})\n",
    "        output_df.to_csv(f\"shap_vs_{feature}.csv\", index=False)\n",
    "\n",
    "        # Bar plot\n",
    "        plt.figure()\n",
    "        plt.bar(output_df[feature].astype(str), output_df[\"mean_shap_value\"])\n",
    "        plt.xlabel(f\"{feature} (0 or 1)\")\n",
    "        plt.ylabel(\"Mean SHAP Value\")\n",
    "        plt.title(f\"SHAP vs. {feature}\")\n",
    "        plt.grid(True, axis=\"y\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        # Continuous feature → bin, filter, and plot\n",
    "        p5, p95 = np.percentile(feature_values, [5, 95])\n",
    "        mask = (feature_values >= p5) & (feature_values <= p95)\n",
    "\n",
    "        filtered_vals = feature_values[mask]\n",
    "        filtered_shap = shap_val[mask]\n",
    "\n",
    "        df_plot = pd.DataFrame({\n",
    "            feature: filtered_vals,\n",
    "            \"shap_value\": filtered_shap\n",
    "        })\n",
    "\n",
    "        bin_edges = np.linspace(filtered_vals.min(), filtered_vals.max(), 20)\n",
    "        df_plot[\"bin\"] = pd.cut(df_plot[feature], bins=bin_edges, include_lowest=True)\n",
    "\n",
    "        bin_means = df_plot.groupby(\"bin\")[\"shap_value\"].mean()\n",
    "        bin_centers = df_plot.groupby(\"bin\")[feature].mean()\n",
    "\n",
    "        output_df = pd.DataFrame({\n",
    "            f\"{feature}_bin_center\": bin_centers,\n",
    "            \"mean_shap_value\": bin_means\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        output_df.to_csv(f\"shap_vs_{feature}.csv\", index=False)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(output_df[f\"{feature}_bin_center\"], output_df[\"mean_shap_value\"], marker=\"o\")\n",
    "        plt.xlabel(f\"{feature} (bin center)\")\n",
    "        plt.ylabel(\"Mean SHAP Value\")\n",
    "        plt.title(f\"SHAP vs. {feature} (5th–95th percentile)\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb052fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze success rate by hour\n",
    "hourly_success = df.groupby('hour_posting')['viral_score'].mean()\n",
    "\n",
    "# Analyze success rate by weekday\n",
    "weekday_success = df.groupby('day_of_week')['viral_score'].mean()\n",
    "\n",
    "# Visualize patterns\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "hourly_success.plot(kind='bar')\n",
    "plt.title('Success Rate by Hour of Day')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "weekday_success.plot(kind='bar')\n",
    "plt.title('Success Rate by Weekday')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from TF-IDF vectorizer\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Find most important text features\n",
    "text_feature_importances = pd.Series(\n",
    "    xgb_cv.feature_importances_[:len(tfidf_feature_names)],\n",
    "    index=tfidf_feature_names\n",
    ").sort_values(ascending=False)[:20]\n",
    "text_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Predict on your cleaned data\n",
    "y_pred = xgb_cv.predict(X_combined)   # or X_test if you want just test set\n",
    "hours  = df_clean['hour_posting']          # same index as X_combined\n",
    "\n",
    "# 2. Build DataFrame of hour vs. prediction\n",
    "df_pred = pd.DataFrame({'hour_posting': hours, 'pred': y_pred})\n",
    "\n",
    "# 3. Compute mean predicted viral_score by hour\n",
    "hour_mean = df_pred.groupby('hour_posting')['pred'].mean().sort_index()\n",
    "\n",
    "# 4. Plot\n",
    "plt.plot(hour_mean.index, hour_mean.values, marker='o')\n",
    "plt.xlabel('Hour of day')\n",
    "plt.ylabel('Average model-predicted viral score')\n",
    "plt.title('Model’s Hour-of-Day Effect')\n",
    "plt.xticks(range(0,24))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944cd081",
   "metadata": {},
   "source": [
    "### Train with is_viral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2bb253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Ensure data sorted by time\n",
    "y = df['is_viral'].reset_index(drop=True)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_combined):\n",
    "    X_train_cv, X_val_cv = X_combined[train_idx], X_combined[val_idx]\n",
    "    y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    xgb_cv = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        tree_method='hist',\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=3.0\n",
    "    )\n",
    "\n",
    "    xgb_cv.fit(X_train_cv, y_train_cv)\n",
    "    y_val_pred = xgb_cv.predict(X_val_cv)\n",
    "    y_val_proba = xgb_cv.predict_proba(X_val_cv)[:, 1]\n",
    "\n",
    "    accuracy_scores.append(accuracy_score(y_val_cv, y_val_pred))\n",
    "    f1_scores.append(f1_score(y_val_cv, y_val_pred))\n",
    "    auc_scores.append(roc_auc_score(y_val_cv, y_val_proba))\n",
    "\n",
    "print(\"Temporal CV Accuracy (mean):\", np.mean(accuracy_scores))\n",
    "print(\"Temporal CV F1 (mean):\", np.mean(f1_scores))\n",
    "print(\"Temporal CV AUC (mean):\", np.mean(auc_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89798605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y is binary (0 or 1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split chronologically (sorted by time)\n",
    "n_total = X_combined.shape[0]\n",
    "split_point = int(n_total * 0.8)\n",
    "train_idx = np.arange(0, split_point)\n",
    "test_idx = np.arange(split_point, n_total)\n",
    "\n",
    "X_train = X_combined[train_idx]\n",
    "X_test = X_combined[test_idx]\n",
    "y_train = df['is_viral'].iloc[train_idx].astype(int)\n",
    "y_test = df['is_viral'].iloc[test_idx].astype(int)\n",
    "\n",
    "# Train final model\n",
    "final_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    scale_pos_weight=2.7\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "y_pred = final_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "\n",
    "# Plot\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c2837",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df = pd.DataFrame(shap_values.values, columns=shap_values.feature_names)\n",
    "mean_abs_shap = shap_df.abs().mean().sort_values(ascending=False)\n",
    "top_features = mean_abs_shap.head(30).index.tolist()  # or any top-N\n",
    "X_train_top = X_train_df[top_features]\n",
    "X_test_top  = X_test_df[top_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27752e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd8d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=3.0,  # adjust for imbalance\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train_top, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d38b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict class probabilities\n",
    "y_proba = xgb.predict_proba(X_test_top)[:, 1]\n",
    "\n",
    "# Choose a threshold (e.g. 0.4 based on F1 curve)\n",
    "threshold = 0.4\n",
    "y_pred = (y_proba > threshold).astype(int)\n",
    "\n",
    "# Print report\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Inverse transform the scaled data\n",
    "X_scaled = additional_data_scaled\n",
    "means  = scaler.mean_\n",
    "scales = scaler.scale_\n",
    "X_orig = X_scaled * scales + means\n",
    "\n",
    "# 2) Wrap in DataFrame\n",
    "df_orig = pd.DataFrame(\n",
    "    X_orig,\n",
    "    columns=additional_cols_final,\n",
    "    index=df_clean.index\n",
    ")\n",
    "\n",
    "# 3) Slice out rows matching train/test split and SHAP inputs\n",
    "df_train_orig = df_orig.loc[X_train_top.index].reset_index(drop=True)\n",
    "df_test_orig  = df_orig.loc[X_test_top.index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74967cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(xgb, X_train_top)\n",
    "shap_values = explainer(X_test_top)\n",
    "\n",
    "shap.plots.bar(shap_values, max_display=20)  # Global importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c22b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=20)  # Dots colored by feature value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = [\"author_fans\", \"video_duration\", \"word_count\", \"isAd\", \"hour_posting\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978af49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features_filtered = relevant_features[relevant_features[\"feature\"].isin(top_features)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare list to store ranges\n",
    "optimal_ranges = []\n",
    "mean_shap_vals = []\n",
    "\n",
    "for feat in top_features:\n",
    "    df_feat = pd.DataFrame({\n",
    "        \"feature_val\": df_test_orig[feat].values,\n",
    "        \"shap_val\": shap_values[:, feat].values\n",
    "    })\n",
    "\n",
    "    # Remove outliers\n",
    "    p1, p99 = np.percentile(df_feat[\"feature_val\"], [1, 99])\n",
    "    df_feat = df_feat[(df_feat[\"feature_val\"] >= p1) & (df_feat[\"feature_val\"] <= p99)]\n",
    "\n",
    "    bins = pd.cut(df_feat[\"feature_val\"], bins=100)\n",
    "    summary = df_feat.groupby(bins, observed=False)[\"shap_val\"].mean()\n",
    "\n",
    "    # Get bin with max SHAP value\n",
    "    if not summary.empty:\n",
    "        best_bin = summary.idxmax()\n",
    "        best_range = f\"{best_bin.left:.1f}–{best_bin.right:.1f}\"\n",
    "    else:\n",
    "        best_range = \"n/a\"\n",
    "\n",
    "    optimal_ranges.append(best_range)\n",
    "    mean_abs_shap = np.abs(shap_values.values[:, X_test_df.columns.get_loc(feat)]).mean()\n",
    "    mean_shap_vals.append(mean_abs_shap)\n",
    "\n",
    "relevant_features_filtered = pd.DataFrame({\n",
    "    \"feature\": top_features,\n",
    "    \"mean_abs_shap\": mean_shap_vals,\n",
    "    \"optimal_value_range\": optimal_ranges\n",
    "})\n",
    "# Add as new column\n",
    "relevant_features_filtered[\"optimal_value_range\"] = optimal_ranges\n",
    "\n",
    "relevant_features_filtered.to_csv(\"successful_post_range_nb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da6f8c",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8540405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# drop all unwanted cols\n",
    "X = df.drop([\n",
    "    'post_id',\n",
    "    'is_viral',\n",
    "    'diggCount',\n",
    "    'shareCount',\n",
    "    'playCount',\n",
    "    'collectCount',\n",
    "    'commentCount',\n",
    "    'engagement',\n",
    "    'searchHashtag_name',\n",
    "    'searchHashtag_views',\n",
    "    'author_nickName',\n",
    "    'detected_language',\n",
    "    'textLanguage',\n",
    "    'date',\n",
    "    'createTimeISO',\n",
    "    'author_signature',\n",
    "    'location_address',\n",
    "    'transcribed_text',\n",
    "    'video_description',\n",
    "    'hashtag_text',\n",
    "    'text_all',\n",
    "    'success_score',\n",
    "    'viral_score',\n",
    "], axis=1)\n",
    "\n",
    "y = df['viral_score']\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(max_features=10000, ngram_range=(1,3)), 'text'),\n",
    "        ('num', StandardScaler(),      ['author_fans','video_duration']),\n",
    "        ('bin', 'passthrough',         ['isAd','isSponsored']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('reg', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=400,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        tree_method='hist',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# CV\n",
    "cv_mae = -cross_val_score(\n",
    "    pipeline, X_train, y_train,\n",
    "    cv=5, scoring='neg_mean_absolute_error', n_jobs=-1\n",
    ").mean()\n",
    "print(f\"CV MAE: {cv_mae:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final fit with early stopping\n",
    "pipeline.fit(\n",
    "    X_train, y_train,\n",
    ")\n",
    "\n",
    "# test MAE\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d662c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"feature_names_in_:\", preprocessor.feature_names_in_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9663fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46488f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab fitted ColumnTransformer\n",
    "pre = pipeline.named_steps['pre']\n",
    "\n",
    "# this array is exactly what the transformer saw when fitting\n",
    "in_feats = pre.feature_names_in_\n",
    "\n",
    "# now request the output names\n",
    "feat_names = pre.get_feature_names_out(input_features=in_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feats = np.array(X_train.columns, dtype=pre.feature_names_in_.dtype)\n",
    "assert np.array_equal(input_feats, pre.feature_names_in_)\n",
    "feat_names = pre.get_feature_names_out(input_features=input_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f95e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "pre = pipeline.named_steps['pre']\n",
    "feat_names = pre.get_feature_names_out()\n",
    "\n",
    "# explainer\n",
    "explainer = shap.Explainer(pipeline.named_steps['reg'], pipeline.named_steps['pre'].transform(X_train))\n",
    "shap_values = explainer(pipeline.named_steps['pre'].transform(X_test))\n",
    "\n",
    "# summary plot: shows feature impact direction\n",
    "shap.summary_plot(shap_values, pipeline.named_steps['pre'].transform(X_test), feat_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 — feature names after all preprocessing\n",
    "feat_names = pipeline.named_steps['pre'].get_feature_names_out()\n",
    "\n",
    "\n",
    "imp = pipeline.named_steps['reg'].feature_importances_\n",
    "\n",
    "# 4 — rank\n",
    "order = np.argsort(imp)          # ascending\n",
    "top_neg = [(feat_names[i], imp[i]) for i in order[:20]]          # worst\n",
    "top_pos = [(feat_names[i], imp[i]) for i in order[-20:][::-1]]   # best\n",
    "\n",
    "print(\"↑ success:\", top_pos)\n",
    "print(\"↓ success:\", top_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Grab the fitted preprocessor and regressor\n",
    "pre = pipeline.named_steps['pre']\n",
    "reg = pipeline.named_steps['reg']\n",
    "\n",
    "# 2. Build the full feature‐name list in the same order the model sees them\n",
    "#   a) text features\n",
    "text_feats = pre.named_transformers_['text'].get_feature_names_out()\n",
    "#   b) numeric + binary features (in the order you passed them)\n",
    "num_feats = ['author_fans', 'video_duration']\n",
    "bin_feats = ['isAd', 'isSponsored']\n",
    "feature_names = np.concatenate([text_feats, num_feats, bin_feats])\n",
    "\n",
    "# 3. Pair names with importances & sort\n",
    "importances = reg.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=feature_names)\n",
    "feat_imp = feat_imp.sort_values(ascending=False)\n",
    "\n",
    "# 4. Show top 10\n",
    "print(feat_imp.head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after model.fit(...)\n",
    "names = model.named_steps['pre'].get_feature_names_out()\n",
    "imp   = model.named_steps['clf'].feature_importances_\n",
    "\n",
    "# sort ascending ⇒ smallest gains first\n",
    "irrelevant = (\n",
    "    pd.Series(imp, index=names)\n",
    "      .sort_values(ascending=True)\n",
    ")\n",
    "\n",
    "# show bottom 30 (near-zero = basically unused)\n",
    "print(irrelevant.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957390f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2 — isolate pieces\n",
    "prep  = pipeline.named_steps['pre']\n",
    "clf   = pipeline.named_steps['clf']\n",
    "names = prep.get_feature_names_out()\n",
    "\n",
    "\n",
    "\n",
    "# 4 — build explainer (LinearExplainer fits logistic/linear models)\n",
    "expl = shap.LinearExplainer(clf, X_tr, feature_names=names)\n",
    "\n",
    "# 5 — global view\n",
    "shap_values = expl(X_va)              # SHAP for validation rows\n",
    "shap.summary_plot(shap_values, X_va, feature_names=names, max_display=25)\n",
    "\n",
    "# 6 — single-post waterfall\n",
    "idx = 0                                # any row index\n",
    "shap.waterfall_plot(shap_values[idx], max_display=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
