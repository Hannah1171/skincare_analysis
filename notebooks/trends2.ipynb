{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11b8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298514bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_transcripts = pd.read_csv(\"../data/raw_transcripts.csv\", encoding='utf-8')\n",
    "df = pd.read_csv(\"../data/comments_posts_transcripts.csv\", encoding='utf-8')\n",
    "\n",
    "df['createTimeISO'] = pd.to_datetime(df['createTimeISO'], utc=True, errors='coerce')\n",
    "\n",
    "three_months_ago = pd.Timestamp.utcnow() - pd.DateOffset(months=3)\n",
    "df = df[df['createTimeISO'] >= three_months_ago]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d391ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def comprehensive_post_aggregation(df):\n",
    "    \"\"\"\n",
    "    Advanced aggregation preserving all TikTok content types\n",
    "    \"\"\"\n",
    "    # Group by post_id with comprehensive feature aggregation\n",
    "    aggregated = df.groupby('post_id').agg({\n",
    "        # Core post content (preserve first occurrence)\n",
    "        'text': 'first',\n",
    "        'transcribed_text': 'first', \n",
    "        'video_description': 'first',\n",
    "        'textLanguage': 'first',\n",
    "        'createTimeISO': 'first',\n",
    "        'author_nickName': 'first',\n",
    "        'author_fans': 'first',\n",
    "        'video_duration': 'first',\n",
    "        \n",
    "        # Engagement metrics\n",
    "        'diggCount': 'first',\n",
    "        'shareCount': 'first', \n",
    "        'playCount': 'first',\n",
    "        'collectCount': 'first',\n",
    "        \n",
    "        # Hashtag information\n",
    "        'searchHashtag_name': 'first',\n",
    "        'searchHashtag_views': 'first',\n",
    "        \n",
    "        # Advanced comment aggregation\n",
    "        'comment': lambda x: ' |COMMENT_SEP| '.join(x.dropna().astype(str)),\n",
    "        'comment_createTimeISO_comment': ['count', 'first', 'last'],\n",
    "        'diggCount_comment': ['sum', 'mean', 'max', 'std'],\n",
    "        'uniqueId_comment': 'nunique',\n",
    "        'comment_lang': lambda x: list(x.dropna().unique())\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    aggregated.columns = [\n",
    "        'post_id', 'text', 'transcribed_text', 'video_description', \n",
    "        'textLanguage', 'createTimeISO', 'author_nickName', 'author_fans',\n",
    "        'video_duration', 'diggCount', 'shareCount', 'playCount', 'collectCount',\n",
    "        'searchHashtag_name', 'searchHashtag_views', 'all_comments',\n",
    "        'comment_count', 'first_comment_time', 'last_comment_time',\n",
    "        'total_comment_likes', 'avg_comment_likes', 'max_comment_likes', 'comment_like_std',\n",
    "        'unique_commenters', 'comment_languages'\n",
    "    ]\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "# Apply aggregation\n",
    "df_posts = comprehensive_post_aggregation(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe89703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hannahschlaucher/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hannahschlaucher/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hannahschlaucher/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/hannahschlaucher/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet', 'vader_lexicon'])\n",
    "\n",
    "def advanced_tiktok_preprocessing(text, handle_slang=True, preserve_emotions=True):\n",
    "    \"\"\"\n",
    "    Enhanced preprocessing for TikTok content with emotion preservation\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase while preserving emotional indicators\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Handle TikTok-specific patterns\n",
    "    text = re.sub(r'@\\w+', '[USER_MENTION]', text)  # Preserve mention structure\n",
    "    text = re.sub(r'#(\\w+)', r'hashtag_\\1', text)   # Convert hashtags to tokens\n",
    "    text = re.sub(r'http\\S+|www\\S+', '[URL]', text) # Replace URLs\n",
    "    \n",
    "    # Handle repeated characters (sooooo -> so) but preserve emphasis\n",
    "    text = re.sub(r'(.)\\1{3,}', r'\\1\\1\\1', text)    # Max 3 repetitions\n",
    "    \n",
    "    # Advanced slang dictionary based on 2024 TikTok trends\n",
    "    if handle_slang:\n",
    "        slang_dict = {\n",
    "            'periodt': 'period', 'slay': 'excellent', 'no cap': 'no lie',\n",
    "            'bussin': 'excellent', 'sheesh': 'impressive', 'mid': 'mediocre',\n",
    "            'bet': 'yes', 'fr': 'for real', 'ong': 'on god', 'ngl': 'not gonna lie',\n",
    "            'lowkey': 'somewhat', 'highkey': 'obviously', 'vibes': 'feelings'\n",
    "        }\n",
    "        for slang, replacement in slang_dict.items():\n",
    "            text = re.sub(r'\\b' + slang + r'\\b', replacement, text)\n",
    "    \n",
    "    # Clean while preserving emotional punctuation\n",
    "    if preserve_emotions:\n",
    "        text = re.sub(r'[^\\w\\s!?.\\']', ' ', text)\n",
    "    else:\n",
    "        text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
    "    \n",
    "    # Tokenization and lemmatization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if len(token) > 1]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply to all text columns\n",
    "text_columns = ['text', 'transcribed_text', 'video_description', 'all_comments']\n",
    "for col in text_columns:\n",
    "    df_posts[f'{col}_processed'] = df_posts[col].apply(advanced_tiktok_preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716c621c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60367e9464884d39aaf5cd2098a25e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 20:18:01.661327: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at j-hartmann/emotion-english-distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Initialize advanced sentiment analyzers\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "emotion_classifier = pipeline(\"text-classification\", \n",
    "                             model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "\n",
    "def comprehensive_sentiment_analysis(text):\n",
    "    \"\"\"\n",
    "    Multi-level sentiment and emotion analysis\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return {\n",
    "            'vader_compound': 0, 'vader_pos': 0, 'vader_neu': 1, 'vader_neg': 0,\n",
    "            'emotion_label': 'neutral', 'emotion_score': 0\n",
    "        }\n",
    "    \n",
    "    # VADER sentiment (optimized for social media)\n",
    "    vader_scores = vader_analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Advanced emotion detection\n",
    "    try:\n",
    "        emotions = emotion_classifier(text[:512])  # Limit for transformer\n",
    "        top_emotion = emotions[0]\n",
    "        emotion_label = top_emotion['label']\n",
    "        emotion_score = top_emotion['score']\n",
    "    except:\n",
    "        emotion_label = 'neutral'\n",
    "        emotion_score = 0\n",
    "    \n",
    "    return {\n",
    "        'vader_compound': vader_scores['compound'],\n",
    "        'vader_pos': vader_scores['pos'],\n",
    "        'vader_neu': vader_scores['neu'], \n",
    "        'vader_neg': vader_scores['neg'],\n",
    "        'emotion_label': emotion_label,\n",
    "        'emotion_score': emotion_score\n",
    "    }\n",
    "\n",
    "# Apply sentiment analysis to all text types\n",
    "for col in text_columns:\n",
    "    sentiment_results = df_posts[f'{col}_processed'].apply(comprehensive_sentiment_analysis)\n",
    "    \n",
    "    # Extract sentiment features\n",
    "    for metric in ['vader_compound', 'vader_pos', 'vader_neu', 'vader_neg', 'emotion_score']:\n",
    "        df_posts[f'{col}_{metric}'] = sentiment_results.apply(lambda x: x[metric])\n",
    "    \n",
    "    df_posts[f'{col}_emotion'] = sentiment_results.apply(lambda x: x['emotion_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a43db4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for text_processed...\n",
      "Generating embeddings for transcribed_text_processed...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "class MultiModalTextEncoder:\n",
    "    def __init__(self):\n",
    "        # Initialize multiple transformer models for ensemble approach\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
    "        \n",
    "        # Set models to evaluation mode\n",
    "        self.bert_model.eval()\n",
    "        self.roberta_model.eval()\n",
    "    \n",
    "    def encode_text_bert(self, texts, max_length=256):\n",
    "        \"\"\"Generate BERT embeddings for text batch\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                if pd.isna(text) or text.strip() == '':\n",
    "                    embeddings.append(np.zeros(768))\n",
    "                    continue\n",
    "                \n",
    "                # Tokenize and encode\n",
    "                encoded = self.bert_tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=max_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # Get embeddings\n",
    "                outputs = self.bert_model(**encoded)\n",
    "                cls_embedding = outputs.last_hidden_state[0][0].numpy()\n",
    "                embeddings.append(cls_embedding)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def encode_text_roberta(self, texts, max_length=256):\n",
    "        \"\"\"Generate RoBERTa embeddings for text batch\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                if pd.isna(text) or text.strip() == '':\n",
    "                    embeddings.append(np.zeros(768))\n",
    "                    continue\n",
    "                \n",
    "                encoded = self.roberta_tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=max_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                outputs = self.roberta_model(**encoded)\n",
    "                cls_embedding = outputs.last_hidden_state[0][0].numpy()\n",
    "                embeddings.append(cls_embedding)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "\n",
    "# Initialize encoder\n",
    "text_encoder = MultiModalTextEncoder()\n",
    "\n",
    "# Generate embeddings for key text features\n",
    "key_text_cols = ['text_processed', 'transcribed_text_processed']\n",
    "\n",
    "for col in key_text_cols:\n",
    "    print(f\"Generating embeddings for {col}...\")\n",
    "    \n",
    "    # BERT embeddings\n",
    "    bert_embeddings = text_encoder.encode_text_bert(df_posts[col].tolist())\n",
    "    df_posts[f'{col}_bert_embedding'] = list(bert_embeddings)\n",
    "    \n",
    "    # RoBERTa embeddings  \n",
    "    roberta_embeddings = text_encoder.encode_text_roberta(df_posts[col].tolist())\n",
    "    df_posts[f'{col}_roberta_embedding'] = list(roberta_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5b0a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 21:39:25,470 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce33db0d2d0b4b58805deb5e59b8b60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 21:40:36,664 - BERTopic - Embedding - Completed ✓\n",
      "2025-06-24 21:40:36,666 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-06-24 21:40:52,280 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-06-24 21:40:52,281 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-06-24 21:40:52,377 - BERTopic - Cluster - Completed ✓\n",
      "2025-06-24 21:40:52,378 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-06-24 21:40:52,996 - BERTopic - Representation - Completed ✓\n",
      "2025-06-24 21:40:52,998 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-06-24 21:40:52,999 - BERTopic - Topic reduction - Number of topics (20) is equal or higher than the clustered topics(12).\n",
      "2025-06-24 21:40:53,000 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-06-24 21:40:53,675 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "def advanced_topic_modeling(texts, n_topics=20):\n",
    "    \"\"\"\n",
    "    Advanced topic modeling using BERTopic\n",
    "    \"\"\"\n",
    "    # Custom vectorizer for TikTok content\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        min_df=5,\n",
    "        max_df=0.95\n",
    "    )\n",
    "    \n",
    "    # Initialize BERTopic with custom parameters\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', prediction_data=True)\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        vectorizer_model=vectorizer,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        nr_topics=n_topics,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Fit model and get topics\n",
    "    topics, probs = topic_model.fit_transform(texts)\n",
    "    \n",
    "    return topic_model, topics, probs\n",
    "\n",
    "def extract_trending_hashtags(hashtag_series):\n",
    "    \"\"\"\n",
    "    Extract and analyze trending hashtag patterns\n",
    "    \"\"\"\n",
    "    # Parse hashtags\n",
    "    all_hashtags = []\n",
    "    hashtag_patterns = []\n",
    "    \n",
    "    for hashtag_string in hashtag_series.dropna():\n",
    "        if isinstance(hashtag_string, str):\n",
    "            tags = re.findall(r'#(\\w+)', hashtag_string.lower())\n",
    "            all_hashtags.extend(tags)\n",
    "            hashtag_patterns.append(tags)\n",
    "    \n",
    "    # Calculate hashtag frequencies and co-occurrences\n",
    "    hashtag_freq = pd.Series(all_hashtags).value_counts()\n",
    "    \n",
    "    # Identify trending patterns\n",
    "    trending_threshold = hashtag_freq.quantile(0.8)\n",
    "    trending_hashtags = hashtag_freq[hashtag_freq >= trending_threshold].index.tolist()\n",
    "    \n",
    "    return hashtag_freq, trending_hashtags, hashtag_patterns\n",
    "\n",
    "# Apply topic modeling\n",
    "combined_text = df_posts['text_processed'].fillna('') + ' ' + df_posts['transcribed_text_processed'].fillna('')\n",
    "topic_model, topics, topic_probs = advanced_topic_modeling(combined_text.tolist())\n",
    "\n",
    "def get_max_prob(p):\n",
    "    if isinstance(p, (list, tuple, np.ndarray)):\n",
    "        return max(p) if len(p) > 0 else 0.0\n",
    "    return float(p)\n",
    "\n",
    "df_posts['primary_topic'] = topics\n",
    "df_posts['topic_probability'] = [get_max_prob(p) for p in topic_probs]\n",
    "\n",
    "# Analyze hashtag trends\n",
    "hashtag_freq, trending_hashtags, hashtag_patterns = extract_trending_hashtags(df_posts['searchHashtag_name'])\n",
    "df_posts['has_trending_hashtag'] = df_posts['searchHashtag_name'].apply(\n",
    "    lambda x: any(tag in trending_hashtags for tag in re.findall(r'#(\\w+)', str(x).lower())) if pd.notna(x) else False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6027bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Advanced temporal feature engineering for trend detection\n",
    "    \"\"\"\n",
    "    # Convert timestamps\n",
    "    df['post_datetime'] = pd.to_datetime(df['createTimeISO'])\n",
    "    df['first_comment_datetime'] = pd.to_datetime(df['first_comment_time'])\n",
    "    df['last_comment_datetime'] = pd.to_datetime(df['last_comment_time'])\n",
    "    \n",
    "    # Basic temporal features\n",
    "    df['hour_of_day'] = df['post_datetime'].dt.hour\n",
    "    df['day_of_week'] = df['post_datetime'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Engagement timing features\n",
    "    df['time_to_first_comment_hours'] = (\n",
    "        df['first_comment_datetime'] - df['post_datetime']\n",
    "    ).dt.total_seconds() / 3600\n",
    "    \n",
    "    df['comment_time_span_hours'] = (\n",
    "        df['last_comment_datetime'] - df['first_comment_datetime']\n",
    "    ).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Velocity features (engagement rate over time)\n",
    "    df['engagement_velocity'] = df['diggCount'] / (df['time_to_first_comment_hours'] + 1)\n",
    "    df['comment_velocity'] = df['comment_count'] / (df['comment_time_span_hours'] + 1)\n",
    "    \n",
    "    # Time-based engagement patterns\n",
    "    df['peak_engagement_hour'] = df.groupby('hour_of_day')['diggCount'].transform('mean')\n",
    "    df['relative_engagement'] = df['diggCount'] / (df['peak_engagement_hour'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply temporal feature engineering\n",
    "df_posts = create_temporal_features(df_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ccc23",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def calculate_viral_indicators(df):\n",
    "    \"\"\"\n",
    "    Calculate advanced viral potential indicators\n",
    "    \"\"\"\n",
    "    # Engagement ratios\n",
    "    df['like_share_ratio'] = df['diggCount'] / (df['shareCount'] + 1)\n",
    "    df['comment_like_ratio'] = df['comment_count'] / (df['diggCount'] + 1)\n",
    "    df['view_engagement_ratio'] = (df['diggCount'] + df['shareCount'] + df['comment_count']) / (df['playCount'] + 1)\n",
    "    \n",
    "    # Author influence factors\n",
    "    df['author_engagement_ratio'] = df['diggCount'] / (df['author_fans'] + 1)\n",
    "    df['follower_amplification'] = df['shareCount'] * np.log1p(df['author_fans'])\n",
    "    \n",
    "    # Community engagement diversity\n",
    "    df['commenter_diversity'] = df['unique_commenters'] / (df['comment_count'] + 1)\n",
    "    df['comment_engagement'] = df['total_comment_likes'] / (df['comment_count'] + 1)\n",
    "    \n",
    "    # Viral potential score (composite metric)\n",
    "    # Normalize features for scoring\n",
    "    engagement_features = ['view_engagement_ratio', 'comment_velocity', 'commenter_diversity', 'relative_engagement']\n",
    "    \n",
    "    for feature in engagement_features:\n",
    "        df[f'{feature}_normalized'] = (df[feature] - df[feature].mean()) / (df[feature].std() + 1e-8)\n",
    "    \n",
    "    df['viral_potential_score'] = (\n",
    "        df['view_engagement_ratio_normalized'] * 0.3 +\n",
    "        df['comment_velocity_normalized'] * 0.25 +\n",
    "        df['commenter_diversity_normalized'] * 0.25 +\n",
    "        df['relative_engagement_normalized'] * 0.2\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate viral indicators\n",
    "df_posts = calculate_viral_indicators(df_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a49c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training individual models...\n",
      "rf: F1 = 0.9975 (+/- 0.0054)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "class TikTokTrendEnsemble:\n",
    "    def __init__(self):\n",
    "        # Base models optimized for social media analysis\n",
    "        self.base_models = {\n",
    "            'rf': RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=5,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'xgb': XGBClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                random_state=42,\n",
    "                eval_metric='logloss'\n",
    "            ),\n",
    "            'lgb': lgb.LGBMClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            ),\n",
    "            'lr': LogisticRegression(\n",
    "                C=1.0,\n",
    "                max_iter=1000,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'svm': SVC(\n",
    "                kernel='rbf',\n",
    "                C=1.0,\n",
    "                probability=True,\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Meta-learning ensemble\n",
    "        self.ensemble = VotingClassifier(\n",
    "            estimators=list(self.base_models.items()),\n",
    "            voting='soft'\n",
    "        )\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"\n",
    "        Prepare feature matrix for training\n",
    "        \"\"\"\n",
    "        # Select numerical features\n",
    "        feature_cols = []\n",
    "        \n",
    "        # Engagement features\n",
    "        engagement_features = [\n",
    "            'diggCount', 'shareCount', 'playCount', 'collectCount', 'comment_count',\n",
    "            'like_share_ratio', 'comment_like_ratio', 'view_engagement_ratio',\n",
    "            'engagement_velocity', 'comment_velocity', 'viral_potential_score'\n",
    "        ]\n",
    "        feature_cols.extend(engagement_features)\n",
    "        \n",
    "        # Temporal features\n",
    "        temporal_features = [\n",
    "            'hour_of_day', 'day_of_week', 'is_weekend',\n",
    "            'time_to_first_comment_hours', 'comment_time_span_hours'\n",
    "        ]\n",
    "        feature_cols.extend(temporal_features)\n",
    "        \n",
    "        # Sentiment features\n",
    "        sentiment_features = []\n",
    "        text_types = ['text', 'transcribed_text', 'all_comments']\n",
    "        for text_type in text_types:\n",
    "            sentiment_features.extend([\n",
    "                f'{text_type}_vader_compound',\n",
    "                f'{text_type}_vader_pos',\n",
    "                f'{text_type}_emotion_score'\n",
    "            ])\n",
    "        feature_cols.extend(sentiment_features)\n",
    "        \n",
    "        # Topic and hashtag features\n",
    "        feature_cols.extend(['topic_probability', 'has_trending_hashtag'])\n",
    "        \n",
    "        # Author features\n",
    "        feature_cols.extend(['author_fans', 'video_duration'])\n",
    "        \n",
    "        # Fill missing values and return feature matrix\n",
    "        feature_matrix = df[feature_cols].fillna(0)\n",
    "        return feature_matrix, feature_cols\n",
    "    \n",
    "    def train(self, df, target_col):\n",
    "        \"\"\"\n",
    "        Train ensemble model\n",
    "        \"\"\"\n",
    "        X, feature_names = self.prepare_features(df)\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Cross-validation evaluation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        print(\"Training individual models...\")\n",
    "        for name, model in self.base_models.items():\n",
    "            scores = cross_val_score(model, X, y, cv=cv, scoring='f1_weighted')\n",
    "            print(f\"{name}: F1 = {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "        \n",
    "        # Train ensemble\n",
    "        print(\"Training ensemble...\")\n",
    "        self.ensemble.fit(X, y)\n",
    "        ensemble_scores = cross_val_score(self.ensemble, X, y, cv=cv, scoring='f1_weighted')\n",
    "        print(f\"Ensemble: F1 = {ensemble_scores.mean():.4f} (+/- {ensemble_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        self.feature_names = feature_names\n",
    "        return self.ensemble\n",
    "\n",
    "# Create target variable (example: high engagement threshold)\n",
    "engagement_threshold = df_posts['viral_potential_score'].quantile(0.8)\n",
    "df_posts['is_trending'] = (df_posts['viral_potential_score'] >= engagement_threshold).astype(int)\n",
    "\n",
    "# Train ensemble model\n",
    "trend_ensemble = TikTokTrendEnsemble()\n",
    "trained_model = trend_ensemble.train(df_posts, 'is_trending')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7945f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 9s 49ms/step - loss: 0.5342 - accuracy: 0.7832 - precision: 0.1250 - recall: 0.0091 - val_loss: 0.4693 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 1s 25ms/step - loss: 0.5187 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4752 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 1s 27ms/step - loss: 0.5147 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4792 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 1s 27ms/step - loss: 0.5148 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4730 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 1s 26ms/step - loss: 0.5127 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4812 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 1s 29ms/step - loss: 0.5131 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4765 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.5150 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4738 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 2s 30ms/step - loss: 0.5146 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4743 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 1s 29ms/step - loss: 0.5102 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4693 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 1s 27ms/step - loss: 0.5081 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4708 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 1s 27ms/step - loss: 0.5129 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4685 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 2s 30ms/step - loss: 0.5095 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4750 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.5102 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4708 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.5084 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4666 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.5097 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4691 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 1s 29ms/step - loss: 0.5069 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4678 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.5085 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4692 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 2s 33ms/step - loss: 0.5072 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4664 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 1s 29ms/step - loss: 0.5076 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4682 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 2s 30ms/step - loss: 0.5078 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4664 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 1s 29ms/step - loss: 0.5100 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4668 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.5121 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4680 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 2s 30ms/step - loss: 0.5052 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4696 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.5061 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4700 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.5084 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4705 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 2s 32ms/step - loss: 0.5113 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4690 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 1s 29ms/step - loss: 0.5054 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4670 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.5051 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4679 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.5084 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4683 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.5066 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4676 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 1s 30ms/step - loss: 0.5083 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4681 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 1s 29ms/step - loss: 0.5072 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4681 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.5036 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4685 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.5061 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4763 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 2s 30ms/step - loss: 0.5059 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4711 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 2s 30ms/step - loss: 0.5039 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4709 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 1s 29ms/step - loss: 0.5052 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4685 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.5050 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4682 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 2s 30ms/step - loss: 0.5050 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4673 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.5044 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4783 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.5077 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4689 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 1s 29ms/step - loss: 0.5035 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4677 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 2s 32ms/step - loss: 0.5057 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4685 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.5073 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4687 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.5086 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4704 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 2s 30ms/step - loss: 0.5052 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4672 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 2s 32ms/step - loss: 0.5061 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4708 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 2s 32ms/step - loss: 0.5042 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4724 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 1s 30ms/step - loss: 0.5056 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4677 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.4995 - accuracy: 0.7945 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4693 - val_accuracy: 0.8225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class TikTokSequenceLSTM:\n",
    "    def __init__(self, sequence_length=24, lstm_units=128):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.lstm_units = lstm_units\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.model = None\n",
    "    \n",
    "    def create_sequences(self, df):\n",
    "        \"\"\"\n",
    "        Create time-based sequences for LSTM training\n",
    "        \"\"\"\n",
    "        # Sort by timestamp\n",
    "        df_sorted = df.sort_values('post_datetime')\n",
    "        \n",
    "        # Select time-series features\n",
    "        ts_features = [\n",
    "            'diggCount', 'shareCount', 'playCount', 'comment_count',\n",
    "            'engagement_velocity', 'viral_potential_score', 'hour_of_day'\n",
    "        ]\n",
    "        \n",
    "        # Normalize features\n",
    "        feature_data = self.scaler.fit_transform(df_sorted[ts_features])\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(self.sequence_length, len(feature_data)):\n",
    "            X.append(feature_data[i-self.sequence_length:i])\n",
    "            y.append(df_sorted.iloc[i]['is_trending'])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Build LSTM model architecture\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(self.lstm_units, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(0.2),\n",
    "            LSTM(self.lstm_units//2, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, df, epochs=50, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train LSTM model\n",
    "        \"\"\"\n",
    "        X, y = self.create_sequences(df)\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            print(\"Not enough data for sequence creation\")\n",
    "            return None\n",
    "        \n",
    "        # Split data\n",
    "        split_idx = int(0.8 * len(X))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        # Build and train model\n",
    "        self.model = self.build_model((X.shape[1], X.shape[2]))\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "# Train LSTM model\n",
    "if len(df_posts) >= 100:  # Ensure sufficient data\n",
    "    lstm_model = TikTokSequenceLSTM()\n",
    "    lstm_history = lstm_model.train(df_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d442246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>transcribed_text</th>\n",
       "      <th>video_description</th>\n",
       "      <th>textLanguage</th>\n",
       "      <th>createTimeISO</th>\n",
       "      <th>author_nickName</th>\n",
       "      <th>author_fans</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>author_engagement_ratio</th>\n",
       "      <th>follower_amplification</th>\n",
       "      <th>commenter_diversity</th>\n",
       "      <th>comment_engagement</th>\n",
       "      <th>view_engagement_ratio_normalized</th>\n",
       "      <th>comment_velocity_normalized</th>\n",
       "      <th>commenter_diversity_normalized</th>\n",
       "      <th>relative_engagement_normalized</th>\n",
       "      <th>viral_potential_score</th>\n",
       "      <th>is_trending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7485370872824417558</td>\n",
       "      <td>#osenrain #skincaredeutschland #kbeautydeutsch...</td>\n",
       "      <td>Baby girl you are so pretty pretty pretty pretty.</td>\n",
       "      <td>This video features K-Beauty toner must-haves....</td>\n",
       "      <td>de</td>\n",
       "      <td>2025-03-24 13:42:04+00:00</td>\n",
       "      <td>osen rain</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4.068966</td>\n",
       "      <td>6.734592</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494036</td>\n",
       "      <td>-0.061246</td>\n",
       "      <td>-3.846459</td>\n",
       "      <td>-0.316443</td>\n",
       "      <td>-0.892004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7485389960384679223</td>\n",
       "      <td>SKINCARE de negrão #skincare #rosto #skincarer...</td>\n",
       "      <td>Girl I wish I could save you but I can't. That...</td>\n",
       "      <td>A young man with a curly afro and a black t-sh...</td>\n",
       "      <td>pt</td>\n",
       "      <td>2025-03-24 14:56:09+00:00</td>\n",
       "      <td>Shaw.</td>\n",
       "      <td>186000</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036650</td>\n",
       "      <td>2305.366393</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>0.661691</td>\n",
       "      <td>-0.079556</td>\n",
       "      <td>-0.954809</td>\n",
       "      <td>-0.262724</td>\n",
       "      <td>-0.112629</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7485395448858037526</td>\n",
       "      <td>Hoffe könnte auch helfen 🙏🏻 | #tiktok #foryou ...</td>\n",
       "      <td>also es ist gefühlt mein dritte oder vierter V...</td>\n",
       "      <td>Ein Video in Deutsch von einer Person, die Tip...</td>\n",
       "      <td>de</td>\n",
       "      <td>2025-03-24 15:17:26+00:00</td>\n",
       "      <td>NRW 📍</td>\n",
       "      <td>263</td>\n",
       "      <td>565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450998</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-2.111469</td>\n",
       "      <td>-0.316864</td>\n",
       "      <td>-0.455946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7485398276494167318</td>\n",
       "      <td>Those were not just freckles bruh 😭 #fypシ #for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-03-24 15:28:25+00:00</td>\n",
       "      <td>Luelle</td>\n",
       "      <td>424000</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>1762.218789</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>-0.836326</td>\n",
       "      <td>-0.081575</td>\n",
       "      <td>0.491016</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>-0.210365</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7485404088989977898</td>\n",
       "      <td>spring self care morning with @felorshop 🌸🧺🧸🩷🌷...</td>\n",
       "      <td>a real tough kid I can handle it They said bab...</td>\n",
       "      <td>No description available.</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-03-24 15:51:33+00:00</td>\n",
       "      <td>adridiaries 💌</td>\n",
       "      <td>518600</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014067</td>\n",
       "      <td>657.944504</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>2.526316</td>\n",
       "      <td>1.623317</td>\n",
       "      <td>-0.145913</td>\n",
       "      <td>1.175881</td>\n",
       "      <td>-0.284394</td>\n",
       "      <td>0.687608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2015</td>\n",
       "      <td>7508580556137925896</td>\n",
       "      <td>3-step morning Korean skincare routine ☀️🌞 #mo...</td>\n",
       "      <td>pretty little baby. You say that maybe you'd b...</td>\n",
       "      <td>Here is a 3-step morning glass skin routine. S...</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-05-26 02:47:31+00:00</td>\n",
       "      <td>IllaAlvarez🎀</td>\n",
       "      <td>154100</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107073</td>\n",
       "      <td>668.940357</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>1.257570</td>\n",
       "      <td>-0.181014</td>\n",
       "      <td>0.491016</td>\n",
       "      <td>-0.282672</td>\n",
       "      <td>0.398237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>2016</td>\n",
       "      <td>7508598335373053227</td>\n",
       "      <td>She made me try her entire skincare routine—an...</td>\n",
       "      <td>Hi. Hi. So, in our first attempt, what are we ...</td>\n",
       "      <td>A woman and a child share tips for a great ski...</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-05-26 03:57:07+00:00</td>\n",
       "      <td>Extension Bar LA</td>\n",
       "      <td>121900</td>\n",
       "      <td>311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>23.421929</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>-0.490797</td>\n",
       "      <td>0.351761</td>\n",
       "      <td>0.491016</td>\n",
       "      <td>-0.317191</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>2017</td>\n",
       "      <td>7508676349607824662</td>\n",
       "      <td>Lasst euch nicht alles andrehen, ihr seid alle...</td>\n",
       "      <td>Viele Leute auf Social Media versuchen euch di...</td>\n",
       "      <td>Die Frau in diesem Video ist im Badezimmer und...</td>\n",
       "      <td>de</td>\n",
       "      <td>2025-05-26 08:59:14+00:00</td>\n",
       "      <td>Leyla Hajrovic</td>\n",
       "      <td>36700</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10.510559</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-0.534642</td>\n",
       "      <td>-0.086204</td>\n",
       "      <td>-0.954809</td>\n",
       "      <td>-0.314350</td>\n",
       "      <td>-0.483516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>2018</td>\n",
       "      <td>7508707415362161942</td>\n",
       "      <td>##beauty #antiaging #glow #skincareroutine #sk...</td>\n",
       "      <td>If you laugh, you can laugh, and you will find...</td>\n",
       "      <td>No description available.</td>\n",
       "      <td>un</td>\n",
       "      <td>2025-05-26 10:59:46+00:00</td>\n",
       "      <td>TamGru</td>\n",
       "      <td>145</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.791881</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-2.111469</td>\n",
       "      <td>-0.317312</td>\n",
       "      <td>-0.828899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>2019</td>\n",
       "      <td>7508707589408967958</td>\n",
       "      <td>[Anzeige] Baumwoll-Pflegetücher und Mandelöl v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>2025-05-26 11:00:25+00:00</td>\n",
       "      <td>🇺🇦Lilia in Berlin🇩🇪</td>\n",
       "      <td>3559</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010393</td>\n",
       "      <td>8.177516</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.470257</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-2.111469</td>\n",
       "      <td>-0.317220</td>\n",
       "      <td>-0.732393</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2020 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0              post_id  \\\n",
       "0              0  7485370872824417558   \n",
       "1              1  7485389960384679223   \n",
       "2              2  7485395448858037526   \n",
       "3              3  7485398276494167318   \n",
       "4              4  7485404088989977898   \n",
       "...          ...                  ...   \n",
       "2015        2015  7508580556137925896   \n",
       "2016        2016  7508598335373053227   \n",
       "2017        2017  7508676349607824662   \n",
       "2018        2018  7508707415362161942   \n",
       "2019        2019  7508707589408967958   \n",
       "\n",
       "                                                   text  \\\n",
       "0     #osenrain #skincaredeutschland #kbeautydeutsch...   \n",
       "1     SKINCARE de negrão #skincare #rosto #skincarer...   \n",
       "2     Hoffe könnte auch helfen 🙏🏻 | #tiktok #foryou ...   \n",
       "3     Those were not just freckles bruh 😭 #fypシ #for...   \n",
       "4     spring self care morning with @felorshop 🌸🧺🧸🩷🌷...   \n",
       "...                                                 ...   \n",
       "2015  3-step morning Korean skincare routine ☀️🌞 #mo...   \n",
       "2016  She made me try her entire skincare routine—an...   \n",
       "2017  Lasst euch nicht alles andrehen, ihr seid alle...   \n",
       "2018  ##beauty #antiaging #glow #skincareroutine #sk...   \n",
       "2019  [Anzeige] Baumwoll-Pflegetücher und Mandelöl v...   \n",
       "\n",
       "                                       transcribed_text  \\\n",
       "0     Baby girl you are so pretty pretty pretty pretty.   \n",
       "1     Girl I wish I could save you but I can't. That...   \n",
       "2     also es ist gefühlt mein dritte oder vierter V...   \n",
       "3                                                   NaN   \n",
       "4     a real tough kid I can handle it They said bab...   \n",
       "...                                                 ...   \n",
       "2015  pretty little baby. You say that maybe you'd b...   \n",
       "2016  Hi. Hi. So, in our first attempt, what are we ...   \n",
       "2017  Viele Leute auf Social Media versuchen euch di...   \n",
       "2018  If you laugh, you can laugh, and you will find...   \n",
       "2019                                                NaN   \n",
       "\n",
       "                                      video_description textLanguage  \\\n",
       "0     This video features K-Beauty toner must-haves....           de   \n",
       "1     A young man with a curly afro and a black t-sh...           pt   \n",
       "2     Ein Video in Deutsch von einer Person, die Tip...           de   \n",
       "3                                                   NaN           en   \n",
       "4                             No description available.           en   \n",
       "...                                                 ...          ...   \n",
       "2015  Here is a 3-step morning glass skin routine. S...           en   \n",
       "2016  A woman and a child share tips for a great ski...           en   \n",
       "2017  Die Frau in diesem Video ist im Badezimmer und...           de   \n",
       "2018                          No description available.           un   \n",
       "2019                                                NaN           de   \n",
       "\n",
       "                  createTimeISO      author_nickName  author_fans  \\\n",
       "0     2025-03-24 13:42:04+00:00            osen rain           28   \n",
       "1     2025-03-24 14:56:09+00:00                Shaw.       186000   \n",
       "2     2025-03-24 15:17:26+00:00                NRW 📍          263   \n",
       "3     2025-03-24 15:28:25+00:00               Luelle       424000   \n",
       "4     2025-03-24 15:51:33+00:00        adridiaries 💌       518600   \n",
       "...                         ...                  ...          ...   \n",
       "2015  2025-05-26 02:47:31+00:00         IllaAlvarez🎀       154100   \n",
       "2016  2025-05-26 03:57:07+00:00     Extension Bar LA       121900   \n",
       "2017  2025-05-26 08:59:14+00:00       Leyla Hajrovic        36700   \n",
       "2018  2025-05-26 10:59:46+00:00               TamGru          145   \n",
       "2019  2025-05-26 11:00:25+00:00  🇺🇦Lilia in Berlin🇩🇪         3559   \n",
       "\n",
       "      video_duration  ...  author_engagement_ratio  follower_amplification  \\\n",
       "0                  4  ...                 4.068966                6.734592   \n",
       "1                 63  ...                 0.036650             2305.366393   \n",
       "2                565  ...                 0.386364                0.000000   \n",
       "3                 11  ...                 0.004278             1762.218789   \n",
       "4                 32  ...                 0.014067              657.944504   \n",
       "...              ...  ...                      ...                     ...   \n",
       "2015              25  ...                 0.107073              668.940357   \n",
       "2016             311  ...                 0.000139               23.421929   \n",
       "2017              55  ...                 0.010000               10.510559   \n",
       "2018              27  ...                 0.013699                0.000000   \n",
       "2019              30  ...                 0.010393                8.177516   \n",
       "\n",
       "      commenter_diversity  comment_engagement  \\\n",
       "0                0.250000            0.000000   \n",
       "1                0.666667            8.666667   \n",
       "2                0.500000            0.000000   \n",
       "3                0.875000            1.750000   \n",
       "4                0.973684            2.526316   \n",
       "...                   ...                 ...   \n",
       "2015             0.875000            2.375000   \n",
       "2016             0.875000            0.875000   \n",
       "2017             0.666667            3.000000   \n",
       "2018             0.500000            0.000000   \n",
       "2019             0.500000            0.500000   \n",
       "\n",
       "     view_engagement_ratio_normalized  comment_velocity_normalized  \\\n",
       "0                            0.494036                    -0.061246   \n",
       "1                            0.661691                    -0.079556   \n",
       "2                            0.450998                    -0.000019   \n",
       "3                           -0.836326                    -0.081575   \n",
       "4                            1.623317                    -0.145913   \n",
       "...                               ...                          ...   \n",
       "2015                         1.257570                    -0.181014   \n",
       "2016                        -0.490797                     0.351761   \n",
       "2017                        -0.534642                    -0.086204   \n",
       "2018                        -0.791881                    -0.000019   \n",
       "2019                        -0.470257                    -0.000019   \n",
       "\n",
       "     commenter_diversity_normalized  relative_engagement_normalized  \\\n",
       "0                         -3.846459                       -0.316443   \n",
       "1                         -0.954809                       -0.262724   \n",
       "2                         -2.111469                       -0.316864   \n",
       "3                          0.491016                       -0.309136   \n",
       "4                          1.175881                       -0.284394   \n",
       "...                             ...                             ...   \n",
       "2015                       0.491016                       -0.282672   \n",
       "2016                       0.491016                       -0.317191   \n",
       "2017                      -0.954809                       -0.314350   \n",
       "2018                      -2.111469                       -0.317312   \n",
       "2019                      -2.111469                       -0.317220   \n",
       "\n",
       "     viral_potential_score is_trending  \n",
       "0                -0.892004           0  \n",
       "1                -0.112629           0  \n",
       "2                -0.455946           0  \n",
       "3                -0.210365           0  \n",
       "4                 0.687608           1  \n",
       "...                    ...         ...  \n",
       "2015              0.398237           1  \n",
       "2016              0.000017           0  \n",
       "2017             -0.483516           0  \n",
       "2018             -0.828899           0  \n",
       "2019             -0.732393           0  \n",
       "\n",
       "[2020 rows x 86 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'post_id', 'text', 'transcribed_text',\n",
       "       'video_description', 'textLanguage', 'createTimeISO', 'author_nickName',\n",
       "       'author_fans', 'video_duration', 'diggCount', 'shareCount', 'playCount',\n",
       "       'collectCount', 'searchHashtag_name', 'searchHashtag_views',\n",
       "       'all_comments', 'comment_count', 'first_comment_time',\n",
       "       'last_comment_time', 'total_comment_likes', 'avg_comment_likes',\n",
       "       'max_comment_likes', 'comment_like_std', 'unique_commenters',\n",
       "       'comment_languages', 'text_processed', 'transcribed_text_processed',\n",
       "       'video_description_processed', 'all_comments_processed',\n",
       "       'text_vader_compound', 'text_vader_pos', 'text_vader_neu',\n",
       "       'text_vader_neg', 'text_emotion_score', 'text_emotion',\n",
       "       'transcribed_text_vader_compound', 'transcribed_text_vader_pos',\n",
       "       'transcribed_text_vader_neu', 'transcribed_text_vader_neg',\n",
       "       'transcribed_text_emotion_score', 'transcribed_text_emotion',\n",
       "       'video_description_vader_compound', 'video_description_vader_pos',\n",
       "       'video_description_vader_neu', 'video_description_vader_neg',\n",
       "       'video_description_emotion_score', 'video_description_emotion',\n",
       "       'all_comments_vader_compound', 'all_comments_vader_pos',\n",
       "       'all_comments_vader_neu', 'all_comments_vader_neg',\n",
       "       'all_comments_emotion_score', 'all_comments_emotion',\n",
       "       'text_processed_bert_embedding', 'text_processed_roberta_embedding',\n",
       "       'transcribed_text_processed_bert_embedding',\n",
       "       'transcribed_text_processed_roberta_embedding', 'primary_topic',\n",
       "       'topic_probability', 'has_trending_hashtag', 'post_datetime',\n",
       "       'first_comment_datetime', 'last_comment_datetime', 'hour_of_day',\n",
       "       'day_of_week', 'is_weekend', 'time_to_first_comment_hours',\n",
       "       'comment_time_span_hours', 'engagement_velocity', 'comment_velocity',\n",
       "       'peak_engagement_hour', 'relative_engagement', 'like_share_ratio',\n",
       "       'comment_like_ratio', 'view_engagement_ratio',\n",
       "       'author_engagement_ratio', 'follower_amplification',\n",
       "       'commenter_diversity', 'comment_engagement',\n",
       "       'view_engagement_ratio_normalized', 'comment_velocity_normalized',\n",
       "       'commenter_diversity_normalized', 'relative_engagement_normalized',\n",
       "       'viral_potential_score', 'is_trending'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    \"\"\"Extract hashtags from text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    hashtags = re.findall(r'#\\w+', text.lower())\n",
    "    return [tag.replace('#', '') for tag in hashtags]\n",
    "\n",
    "# Extract hashtags\n",
    "df_posts['hashtag_list'] = df_posts['text'].apply(extract_hashtags)\n",
    "df_posts['hashtag_text'] = df_posts['hashtag_list'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67297938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hashtags\n",
    "df_posts['hashtag_list'] = df_posts['text'].apply(extract_hashtags)\n",
    "df_posts['hashtag_text'] = df_posts['hashtag_list'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff473e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      viral_potential_score  primary_topic  \\\n",
      "4                  0.687608             11   \n",
      "5                  0.722942              7   \n",
      "13                 0.801682             -1   \n",
      "15                 0.746672             11   \n",
      "20                 0.806918              9   \n",
      "...                     ...            ...   \n",
      "1992               0.487884              9   \n",
      "2007               0.549889             -1   \n",
      "2013               1.296288              7   \n",
      "2014               0.645447              9   \n",
      "2015               0.398237              8   \n",
      "\n",
      "                                           hashtag_list  \n",
      "4     [felorshop, amazonfinds, skincareroutine, face...  \n",
      "5     [medicube, medicubeskincare, medicubedevice, s...  \n",
      "13    [skincare, grwm, storytime, creatorsearchinsig...  \n",
      "15    [vlog, stdwm, roadtrip, skincareroutine, trave...  \n",
      "20    [morning, morningroutine, morninginmylife, dit...  \n",
      "...                                                 ...  \n",
      "1992  [asmr, asmrroutine, morningroutine, mymorning,...  \n",
      "2007  [grwm, grwm, makeup, skincare, curlyhair, curl...  \n",
      "2013  [capcut, skincareroutine, selfcare, skintok, m...  \n",
      "2014  [girlytok, mircoinfluencer, prpackage, trends,...  \n",
      "2015  [morningskincare, morningroutine, morningskinc...  \n",
      "\n",
      "[404 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example: Show top 10 trending posts\n",
    "top_trends = df_posts[df_posts['is_trending']==True]\n",
    "print(top_trends[['viral_potential_score', 'primary_topic', 'hashtag_list']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7df1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      viral_potential_score  primary_topic  \\\n",
      "735                6.120116              9   \n",
      "1616               4.622916              5   \n",
      "1442               4.549828              7   \n",
      "678                3.364549             -1   \n",
      "189                3.244957              9   \n",
      "1907               2.806141              7   \n",
      "348                2.493393              7   \n",
      "269                2.462886              9   \n",
      "829                2.259459             -1   \n",
      "369                2.042982              9   \n",
      "340                2.026935             10   \n",
      "287                1.959911              9   \n",
      "712                1.943716              9   \n",
      "98                 1.932193             10   \n",
      "1215               1.886525              4   \n",
      "1787               1.862344              4   \n",
      "1620               1.825779              9   \n",
      "584                1.664783              9   \n",
      "1360               1.663104             -1   \n",
      "801                1.662904              7   \n",
      "647                1.654070             -1   \n",
      "907                1.621654              9   \n",
      "450                1.393781             -1   \n",
      "182                1.379612              7   \n",
      "1591               1.338062              7   \n",
      "1353               1.319964              9   \n",
      "257                1.310625             -1   \n",
      "273                1.303157             -1   \n",
      "2013               1.296288              7   \n",
      "118                1.288379              9   \n",
      "27                 1.283043             10   \n",
      "275                1.281152             -1   \n",
      "863                1.263622              9   \n",
      "201                1.258693              7   \n",
      "143                1.252939              9   \n",
      "1734               1.249848             -1   \n",
      "1057               1.237368              7   \n",
      "53                 1.141069              9   \n",
      "1164               1.134362              7   \n",
      "729                1.129471              9   \n",
      "843                1.118452             -1   \n",
      "341                1.106705              9   \n",
      "999                1.104127              9   \n",
      "322                1.103088              9   \n",
      "918                1.073824              9   \n",
      "1766               1.058159             -1   \n",
      "129                1.049203              9   \n",
      "1348               1.039550              9   \n",
      "113                1.036129              7   \n",
      "1240               1.024427             11   \n",
      "\n",
      "                                           hashtag_list  \n",
      "735   [skincare, skincareroutine, skincareproducts, ...  \n",
      "1616  [selbstfürsorge, mentalhealth, skincareundmind...  \n",
      "1442  [dermatologist, skincare, tea, teatime, storyt...  \n",
      "678   [april, aprilempties, empty, empties, thismont...  \n",
      "189   [fyp, skincare, skintok, koreanskincare, 7days...  \n",
      "1907  [bodycare, skincare, skincareroutine, skincare...  \n",
      "348                         [medicube, medicubepartner]  \n",
      "269   [fyp, 2020, vsco, viral, aesthetics, closeup, ...  \n",
      "829   [sunburnrelief, suntips, sunsafety, skincareti...  \n",
      "369   [kiehlsfrance, nyxprofessionalmakeup, grwm, gr...  \n",
      "340        [storytime, story, skincare, morningroutine]  \n",
      "287   [creatorsearchinsights, koreanskincare, koran,...  \n",
      "712   [darkspots, hyperpigmentation, koreanskincare,...  \n",
      "98                             [grwm, makeup, skincare]  \n",
      "1215  [escuela, grwm, rutinaescuela, alistateconmigo...  \n",
      "1787  [asmrskincare, skincare, skincareroutine, skin...  \n",
      "1620  [selfcare, koreanskincare, videodiary, aesthet...  \n",
      "584   [baby, babygirl, babytok, babyfever, babytikto...  \n",
      "1360  [koreanskincare, facewash, facecleanser, skinc...  \n",
      "801   [beautytips, viral, fypage, morningshed, befor...  \n",
      "647   [creatorsearchinsights, epilator, hairremoval,...  \n",
      "907   [eyepatches, nightskincareroutine, charlotteti...  \n",
      "450                             [greenscreen, skincare]  \n",
      "182                      [gurwm, skincare, fyp, foryou]  \n",
      "1591                                         [skincare]  \n",
      "1353  [viral, canopy, room, nightroutine, skincare, ...  \n",
      "257   [operationolivebranch, filters, effects, effec...  \n",
      "273   [operationolivebranch, filters, korea, korean,...  \n",
      "2013  [capcut, skincareroutine, selfcare, skintok, m...  \n",
      "118   [grwmforpookies, makeuptalk, grwmstorytime, sk...  \n",
      "27    [grwm, getreadywithme, skincare, makeup, d1hat...  \n",
      "275   [koreanskincare, facecleanser, facewash, skinc...  \n",
      "863   [라네즈, 신제품, 스킨케어, laneige, newproduct, skincare...  \n",
      "201   [mikaylanogueira, pov, skincare, fyp, grwm, sk...  \n",
      "143   [skin1004, pinkstick, poremizing, pores, mask,...  \n",
      "1734  [haileybieber, rhodeskin, skincare, skincarero...  \n",
      "1057  [glowup, skincare, skincareroutine, morningski...  \n",
      "53    [morningshed, medicube, pdrncapsulecream, salm...  \n",
      "1164  [hydrationhero, wnpfacewash, skincare, skincar...  \n",
      "729       [シミ取り16日後, nightcare, skincare, asmr, 色素沈着ケア]  \n",
      "843   [skincare, skincareroutines, travelsize, trave...  \n",
      "341   [anua, skincare, glowup, koreanskincare, acnep...  \n",
      "999   [asmr, selfcaremorning, morningroutine, bodyca...  \n",
      "322                [asmr, skincare, facial, relaxation]  \n",
      "918   [seoul, seoulsouthkorea, korea, southkorea, so...  \n",
      "1766  [jadefaceroller, faceroller, skincare, darkcir...  \n",
      "129   [asmr, asmrskincare, skincare, skincareroutine...  \n",
      "1348   [outfit, ootd, makeup, grwm, skincare, haircare]  \n",
      "113                 [medicube, pdrn, skincare, gelmask]  \n",
      "1240  [pinkroutine, morningvibes, morningroutine, as...  \n"
     ]
    }
   ],
   "source": [
    "# Example: Show top 10 trending posts\n",
    "top_trends = df_posts.sort_values('viral_potential_score', ascending=False).head(50)\n",
    "print(top_trends[['viral_potential_score', 'primary_topic', 'hashtag_list']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d15134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('skincare', 332),\n",
       " ('skincareroutine', 153),\n",
       " ('fyp', 128),\n",
       " ('grwm', 108),\n",
       " ('koreanskincare', 74),\n",
       " ('makeup', 56),\n",
       " ('selfcare', 52),\n",
       " ('kbeauty', 50),\n",
       " ('morningroutine', 43),\n",
       " ('viral', 42),\n",
       " ('skincaretips', 34),\n",
       " ('asmr', 32),\n",
       " ('nightroutine', 29),\n",
       " ('foryoupage', 26),\n",
       " ('fypシ', 26),\n",
       " ('aesthetic', 25),\n",
       " ('routine', 25),\n",
       " ('creatorsearchinsights', 24),\n",
       " ('trending', 23),\n",
       " ('getreadywithme', 22),\n",
       " ('foryou', 22),\n",
       " ('beauty', 22),\n",
       " ('glowup', 22),\n",
       " ('gurwm', 21),\n",
       " ('morning', 20),\n",
       " ('skintok', 20),\n",
       " ('glassskin', 19),\n",
       " ('acne', 18),\n",
       " ('medicube', 16),\n",
       " ('grwmroutine', 16)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "all_hashtags = list(itertools.chain.from_iterable(top_trends['hashtag_list']))\n",
    "hashtag_counts = Counter(all_hashtags)\n",
    "top_hashtags = hashtag_counts.most_common(30)\n",
    "\n",
    "top_hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d65d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# If you used BERTopic\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241m.\u001b[39mget_topic_info()  \u001b[38;5;66;03m# Shows topic numbers and their top words\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# To get a human-readable label for a topic:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic_num \u001b[38;5;129;01min\u001b[39;00m trending_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_topic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topic_model' is not defined"
     ]
    }
   ],
   "source": [
    "# If you used BERTopic\n",
    "topic_model.get_topic_info()  # Shows topic numbers and their top words\n",
    "\n",
    "# To get a human-readable label for a topic:\n",
    "for topic_num in trending_posts['primary_topic'].unique():\n",
    "    print(f\"Topic {topic_num}: {topic_model.get_topic(topic_num)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c04b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='primary_topic', data=top_trends)\n",
    "plt.title('Primary Topics of Trending Posts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d24e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       330\n",
      "           1       1.00      1.00      1.00        74\n",
      "\n",
      "    accuracy                           1.00       404\n",
      "   macro avg       1.00      1.00      1.00       404\n",
      "weighted avg       1.00      1.00      1.00       404\n",
      "\n",
      "\n",
      "ROC AUC Score: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def comprehensive_model_evaluation(model, X_test, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of trend detection model\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Classification metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # ROC AUC\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"\\nROC AUC Score: {auc_score:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance (for tree-based models)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(data=feature_imp.head(20), x='importance', y='feature')\n",
    "        plt.title('Top 20 Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'auc': auc_score,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "X_features, feature_names = trend_ensemble.prepare_features(df_posts)\n",
    "train_size = int(0.8 * len(X_features))\n",
    "X_test = X_features[train_size:]\n",
    "y_test = df_posts['is_trending'].iloc[train_size:]\n",
    "\n",
    "evaluation_results = comprehensive_model_evaluation(\n",
    "    trained_model, X_test, y_test, feature_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ca5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    \"\"\"Extract hashtags from text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    hashtags = re.findall(r'#\\w+', text.lower())\n",
    "    return [tag.replace('#', '') for tag in hashtags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d32044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hashtags\n",
    "df['hashtag_list'] = df['text'].apply(extract_hashtags)\n",
    "df['hashtag_text'] = df['hashtag_list'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36caeea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_id', 'text', 'transcribed_text', 'video_description',\n",
       "       'textLanguage', 'detected_language', 'createTimeISO', 'isAd',\n",
       "       'isSponsored', 'video_duration', 'date', 'author_nickName',\n",
       "       'author_signature', 'author_fans', 'diggCount', 'shareCount',\n",
       "       'playCount', 'collectCount', 'commentCount', 'searchHashtag_name',\n",
       "       'searchHashtag_views', 'location_address', 'all_comments_text',\n",
       "       'total_comments', 'first_comment_time', 'last_comment_time',\n",
       "       'total_comment_likes', 'avg_comment_likes', 'max_comment_likes',\n",
       "       'total_comment_replies', 'avg_comment_replies', 'unique_commenters',\n",
       "       'comment_languages', 'comment_entries_count', 'hashtag_list',\n",
       "       'hashtag_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ec650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def create_tfidf_features(text_series, max_features=5000, ngram_range=(1,2)):\n",
    "    \"\"\"\n",
    "    Create TF-IDF features for text data\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        strip_accents='unicode',\n",
    "        lowercase=True\n",
    "    )\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_series.fillna(''))\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    return tfidf_matrix, vectorizer, feature_names\n",
    "\n",
    "# Create TF-IDF features for different text types\n",
    "transcript_tfidf, transcript_vectorizer, transcript_features = create_tfidf_features(df['transcribed_text'])\n",
    "comments_tfidf, comments_vectorizer, comments_features = create_tfidf_features(df['all_comments_text'])\n",
    "captions_tfidf, captions_vectorizer, captions_features = create_tfidf_features(df['text'])\n",
    "hashtag_tfidf, hashtag_vectorizer, hashtag_features = create_tfidf_features(df['hashtag_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "\n",
    "def create_word2vec_embeddings(text_series, vector_size=100, window=5, min_count=2):\n",
    "    \"\"\"\n",
    "    Create Word2Vec embeddings\n",
    "    \"\"\"\n",
    "    # Prepare sentences for Word2Vec\n",
    "    sentences = [simple_preprocess(text) for text in text_series.fillna('') if text.strip()]\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=4,\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def text_to_vector(text, model, vector_size=100):\n",
    "    \"\"\"\n",
    "    Convert text to average word vector\n",
    "    \"\"\"\n",
    "    words = simple_preprocess(text)\n",
    "    word_vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word])\n",
    "    \n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Create Word2Vec embeddings\n",
    "combined_text = pd.concat([\n",
    "    df['transcribed_text'], \n",
    "    df['comment'], \n",
    "    df['text']\n",
    "]).dropna()\n",
    "\n",
    "word2vec_model = create_word2vec_embeddings(combined_text)\n",
    "\n",
    "# Generate document vectors\n",
    "df['transcript_w2v'] = df['transcribed_text'].apply(\n",
    "    lambda x: text_to_vector(x, word2vec_model) if pd.notna(x) else np.zeros(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488fada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a479c6a373a14bce812e7aec1816c699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66aaa78707d44276a705e320014a67ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21ce5b7cdad499ca0a77a994c1d8200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adf46de3dd541529d190d2f52feff84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c1cce3a5f44686bcdfd952156d0963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df), batch_size):\n\u001b[1;32m     44\u001b[0m     batch_texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscribed_text\u001b[39m\u001b[38;5;124m'\u001b[39m][i:i\u001b[38;5;241m+\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 45\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_bert_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     bert_embeddings\u001b[38;5;241m.\u001b[39mextend(batch_embeddings)\n\u001b[1;32m     48\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(bert_embeddings)\n",
      "Cell \u001b[0;32mIn[24], line 31\u001b[0m, in \u001b[0;36mcreate_bert_embeddings\u001b[0;34m(texts, model_name, max_length)\u001b[0m\n\u001b[1;32m     21\u001b[0m encoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m     22\u001b[0m     text,\n\u001b[1;32m     23\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Get BERT embeddings\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Use [CLS] token embedding as document representation\u001b[39;00m\n\u001b[1;32m     34\u001b[0m cls_embedding \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1144\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1144\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1156\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1157\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:395\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    384\u001b[0m         hidden_states,\n\u001b[1;32m    385\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         output_attentions,\n\u001b[1;32m    391\u001b[0m     )\n\u001b[1;32m    393\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 395\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/skincare-analysis-pFfgqJ8c-py3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "def create_bert_embeddings(texts, model_name='bert-base-uncased', max_length=512):\n",
    "    \"\"\"\n",
    "    Create BERT embeddings for text data\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            if pd.isna(text) or text.strip() == '':\n",
    "                embeddings.append(np.zeros(768))  # BERT base has 768 dimensions\n",
    "                continue\n",
    "                \n",
    "            # Tokenize and encode\n",
    "            encoded = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Get BERT embeddings\n",
    "            outputs = model(**encoded)\n",
    "            \n",
    "            # Use [CLS] token embedding as document representation\n",
    "            cls_embedding = outputs.last_hidden_state[0][0].numpy()\n",
    "            embeddings.append(cls_embedding)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Create BERT embeddings (use smaller batches to manage memory)\n",
    "batch_size = 32\n",
    "bert_embeddings = []\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['transcribed_text'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = create_bert_embeddings(batch_texts)\n",
    "    bert_embeddings.extend(batch_embeddings)\n",
    "\n",
    "df['bert_embeddings'] = list(bert_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skincare-analysis-pFfgqJ8c-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
