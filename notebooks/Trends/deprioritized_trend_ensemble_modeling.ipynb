{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298514bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/comments_posts_transcripts.csv\", encoding='utf-8')\n",
    "\n",
    "df['createTimeISO'] = pd.to_datetime(df['createTimeISO'], utc=True, errors='coerce')\n",
    "\n",
    "three_months_ago = pd.Timestamp.utcnow() - pd.DateOffset(months=3)\n",
    "df = df[df['createTimeISO'] >= three_months_ago]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d391ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_post_aggregation(df):\n",
    "    \"\"\"\n",
    "    Advanced aggregation preserving all TikTok content types\n",
    "    \"\"\"\n",
    "    # Group by post_id with comprehensive feature aggregation\n",
    "    aggregated = df.groupby('post_id').agg({\n",
    "        # Core post content (preserve first occurrence)\n",
    "        'text': 'first',\n",
    "        'transcribed_text': 'first', \n",
    "        'video_description': 'first',\n",
    "        'textLanguage': 'first',\n",
    "        'createTimeISO': 'first',\n",
    "        'author_nickName': 'first',\n",
    "        'author_fans': 'first',\n",
    "        'video_duration': 'first',\n",
    "        \n",
    "        # Engagement metrics\n",
    "        'diggCount': 'first',\n",
    "        'shareCount': 'first', \n",
    "        'playCount': 'first',\n",
    "        'collectCount': 'first',\n",
    "        \n",
    "        # Hashtag information\n",
    "        'searchHashtag_name': 'first',\n",
    "        'searchHashtag_views': 'first',\n",
    "        \n",
    "        # Advanced comment aggregation\n",
    "        'comment': lambda x: ' |COMMENT_SEP| '.join(x.dropna().astype(str)),\n",
    "        'comment_createTimeISO_comment': ['count', 'first', 'last'],\n",
    "        'diggCount_comment': ['sum', 'mean', 'max', 'std'],\n",
    "        'uniqueId_comment': 'nunique',\n",
    "        'comment_lang': lambda x: list(x.dropna().unique())\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    aggregated.columns = [\n",
    "        'post_id', 'text', 'transcribed_text', 'video_description', \n",
    "        'textLanguage', 'createTimeISO', 'author_nickName', 'author_fans',\n",
    "        'video_duration', 'diggCount', 'shareCount', 'playCount', 'collectCount',\n",
    "        'searchHashtag_name', 'searchHashtag_views', 'all_comments',\n",
    "        'comment_count', 'first_comment_time', 'last_comment_time',\n",
    "        'total_comment_likes', 'avg_comment_likes', 'max_comment_likes', 'comment_like_std',\n",
    "        'unique_commenters', 'comment_languages'\n",
    "    ]\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "# Apply aggregation\n",
    "df_posts = comprehensive_post_aggregation(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe89703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet', 'vader_lexicon'])\n",
    "\n",
    "def advanced_tiktok_preprocessing(text, handle_slang=True, preserve_emotions=True):\n",
    "    \"\"\"\n",
    "    Enhanced preprocessing for TikTok content with emotion preservation\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase while preserving emotional indicators\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Handle TikTok-specific patterns\n",
    "    text = re.sub(r'@\\w+', '[USER_MENTION]', text)  # Preserve mention structure\n",
    "    text = re.sub(r'#(\\w+)', r'hashtag_\\1', text)   # Convert hashtags to tokens\n",
    "    text = re.sub(r'http\\S+|www\\S+', '[URL]', text) # Replace URLs\n",
    "    \n",
    "    # Handle repeated characters (sooooo -> so) but preserve emphasis\n",
    "    text = re.sub(r'(.)\\1{3,}', r'\\1\\1\\1', text)    # Max 3 repetitions\n",
    "    \n",
    "    # Advanced slang dictionary based on 2024 TikTok trends\n",
    "    if handle_slang:\n",
    "        slang_dict = {\n",
    "            'periodt': 'period', 'slay': 'excellent', 'no cap': 'no lie',\n",
    "            'bussin': 'excellent', 'sheesh': 'impressive', 'mid': 'mediocre',\n",
    "            'bet': 'yes', 'fr': 'for real', 'ong': 'on god', 'ngl': 'not gonna lie',\n",
    "            'lowkey': 'somewhat', 'highkey': 'obviously', 'vibes': 'feelings'\n",
    "        }\n",
    "        for slang, replacement in slang_dict.items():\n",
    "            text = re.sub(r'\\b' + slang + r'\\b', replacement, text)\n",
    "    \n",
    "    # Clean while preserving emotional punctuation\n",
    "    if preserve_emotions:\n",
    "        text = re.sub(r'[^\\w\\s!?.\\']', ' ', text)\n",
    "    else:\n",
    "        text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
    "    \n",
    "    # Tokenization and lemmatization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if len(token) > 1]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply to all text columns\n",
    "text_columns = ['text', 'transcribed_text', 'video_description', 'all_comments']\n",
    "for col in text_columns:\n",
    "    df_posts[f'{col}_processed'] = df_posts[col].apply(advanced_tiktok_preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Initialize advanced sentiment analyzers\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "emotion_classifier = pipeline(\"text-classification\", \n",
    "                             model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "\n",
    "def comprehensive_sentiment_analysis(text):\n",
    "    \"\"\"\n",
    "    Multi-level sentiment and emotion analysis\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return {\n",
    "            'vader_compound': 0, 'vader_pos': 0, 'vader_neu': 1, 'vader_neg': 0,\n",
    "            'emotion_label': 'neutral', 'emotion_score': 0\n",
    "        }\n",
    "    \n",
    "    # VADER sentiment (optimized for social media)\n",
    "    vader_scores = vader_analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Advanced emotion detection\n",
    "    try:\n",
    "        emotions = emotion_classifier(text[:512])  # Limit for transformer\n",
    "        top_emotion = emotions[0]\n",
    "        emotion_label = top_emotion['label']\n",
    "        emotion_score = top_emotion['score']\n",
    "    except:\n",
    "        emotion_label = 'neutral'\n",
    "        emotion_score = 0\n",
    "    \n",
    "    return {\n",
    "        'vader_compound': vader_scores['compound'],\n",
    "        'vader_pos': vader_scores['pos'],\n",
    "        'vader_neu': vader_scores['neu'], \n",
    "        'vader_neg': vader_scores['neg'],\n",
    "        'emotion_label': emotion_label,\n",
    "        'emotion_score': emotion_score\n",
    "    }\n",
    "\n",
    "# Apply sentiment analysis to all text types\n",
    "for col in text_columns:\n",
    "    sentiment_results = df_posts[f'{col}_processed'].apply(comprehensive_sentiment_analysis)\n",
    "    \n",
    "    # Extract sentiment features\n",
    "    for metric in ['vader_compound', 'vader_pos', 'vader_neu', 'vader_neg', 'emotion_score']:\n",
    "        df_posts[f'{col}_{metric}'] = sentiment_results.apply(lambda x: x[metric])\n",
    "    \n",
    "    df_posts[f'{col}_emotion'] = sentiment_results.apply(lambda x: x['emotion_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43db4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "class MultiModalTextEncoder:\n",
    "    def __init__(self):\n",
    "        # Initialize multiple transformer models for ensemble approach\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
    "        \n",
    "        # Set models to evaluation mode\n",
    "        self.bert_model.eval()\n",
    "        self.roberta_model.eval()\n",
    "    \n",
    "    def encode_text_bert(self, texts, max_length=256):\n",
    "        \"\"\"Generate BERT embeddings for text batch\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                if pd.isna(text) or text.strip() == '':\n",
    "                    embeddings.append(np.zeros(768))\n",
    "                    continue\n",
    "                \n",
    "                # Tokenize and encode\n",
    "                encoded = self.bert_tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=max_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # Get embeddings\n",
    "                outputs = self.bert_model(**encoded)\n",
    "                cls_embedding = outputs.last_hidden_state[0][0].numpy()\n",
    "                embeddings.append(cls_embedding)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def encode_text_roberta(self, texts, max_length=256):\n",
    "        \"\"\"Generate RoBERTa embeddings for text batch\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                if pd.isna(text) or text.strip() == '':\n",
    "                    embeddings.append(np.zeros(768))\n",
    "                    continue\n",
    "                \n",
    "                encoded = self.roberta_tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=max_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                outputs = self.roberta_model(**encoded)\n",
    "                cls_embedding = outputs.last_hidden_state[0][0].numpy()\n",
    "                embeddings.append(cls_embedding)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "\n",
    "# Initialize encoder\n",
    "text_encoder = MultiModalTextEncoder()\n",
    "\n",
    "# Generate embeddings for key text features\n",
    "key_text_cols = ['text_processed', 'transcribed_text_processed']\n",
    "\n",
    "for col in key_text_cols:\n",
    "    print(f\"Generating embeddings for {col}...\")\n",
    "    \n",
    "    # BERT embeddings\n",
    "    bert_embeddings = text_encoder.encode_text_bert(df_posts[col].tolist())\n",
    "    df_posts[f'{col}_bert_embedding'] = list(bert_embeddings)\n",
    "    \n",
    "    # RoBERTa embeddings  \n",
    "    roberta_embeddings = text_encoder.encode_text_roberta(df_posts[col].tolist())\n",
    "    df_posts[f'{col}_roberta_embedding'] = list(roberta_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "def advanced_topic_modeling(texts, n_topics=20):\n",
    "    \"\"\"\n",
    "    Advanced topic modeling using BERTopic\n",
    "    \"\"\"\n",
    "    # Custom vectorizer for TikTok content\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        min_df=5,\n",
    "        max_df=0.95\n",
    "    )\n",
    "    \n",
    "    # Initialize BERTopic with custom parameters\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', prediction_data=True)\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        vectorizer_model=vectorizer,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        nr_topics=n_topics,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Fit model and get topics\n",
    "    topics, probs = topic_model.fit_transform(texts)\n",
    "    \n",
    "    return topic_model, topics, probs\n",
    "\n",
    "def extract_trending_hashtags(hashtag_series):\n",
    "    \"\"\"\n",
    "    Extract and analyze trending hashtag patterns\n",
    "    \"\"\"\n",
    "    # Parse hashtags\n",
    "    all_hashtags = []\n",
    "    hashtag_patterns = []\n",
    "    \n",
    "    for hashtag_string in hashtag_series.dropna():\n",
    "        if isinstance(hashtag_string, str):\n",
    "            tags = re.findall(r'#(\\w+)', hashtag_string.lower())\n",
    "            all_hashtags.extend(tags)\n",
    "            hashtag_patterns.append(tags)\n",
    "    \n",
    "    # Calculate hashtag frequencies and co-occurrences\n",
    "    hashtag_freq = pd.Series(all_hashtags).value_counts()\n",
    "    \n",
    "    # Identify trending patterns\n",
    "    trending_threshold = hashtag_freq.quantile(0.8)\n",
    "    trending_hashtags = hashtag_freq[hashtag_freq >= trending_threshold].index.tolist()\n",
    "    \n",
    "    return hashtag_freq, trending_hashtags, hashtag_patterns\n",
    "\n",
    "# Apply topic modeling\n",
    "combined_text = df_posts['text_processed'].fillna('') + ' ' + df_posts['transcribed_text_processed'].fillna('')\n",
    "topic_model, topics, topic_probs = advanced_topic_modeling(combined_text.tolist())\n",
    "\n",
    "def get_max_prob(p):\n",
    "    if isinstance(p, (list, tuple, np.ndarray)):\n",
    "        return max(p) if len(p) > 0 else 0.0\n",
    "    return float(p)\n",
    "\n",
    "df_posts['primary_topic'] = topics\n",
    "df_posts['topic_probability'] = [get_max_prob(p) for p in topic_probs]\n",
    "\n",
    "# Analyze hashtag trends\n",
    "hashtag_freq, trending_hashtags, hashtag_patterns = extract_trending_hashtags(df_posts['searchHashtag_name'])\n",
    "df_posts['has_trending_hashtag'] = df_posts['searchHashtag_name'].apply(\n",
    "    lambda x: any(tag in trending_hashtags for tag in re.findall(r'#(\\w+)', str(x).lower())) if pd.notna(x) else False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6027bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Advanced temporal feature engineering for trend detection\n",
    "    \"\"\"\n",
    "    # Convert timestamps\n",
    "    df['post_datetime'] = pd.to_datetime(df['createTimeISO'])\n",
    "    df['first_comment_datetime'] = pd.to_datetime(df['first_comment_time'])\n",
    "    df['last_comment_datetime'] = pd.to_datetime(df['last_comment_time'])\n",
    "    \n",
    "    # Basic temporal features\n",
    "    df['hour_of_day'] = df['post_datetime'].dt.hour\n",
    "    df['day_of_week'] = df['post_datetime'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Engagement timing features\n",
    "    df['time_to_first_comment_hours'] = (\n",
    "        df['first_comment_datetime'] - df['post_datetime']\n",
    "    ).dt.total_seconds() / 3600\n",
    "    \n",
    "    df['comment_time_span_hours'] = (\n",
    "        df['last_comment_datetime'] - df['first_comment_datetime']\n",
    "    ).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Velocity features (engagement rate over time)\n",
    "    df['engagement_velocity'] = df['diggCount'] / (df['time_to_first_comment_hours'] + 1)\n",
    "    df['comment_velocity'] = df['comment_count'] / (df['comment_time_span_hours'] + 1)\n",
    "    \n",
    "    # Time-based engagement patterns\n",
    "    df['peak_engagement_hour'] = df.groupby('hour_of_day')['diggCount'].transform('mean')\n",
    "    df['relative_engagement'] = df['diggCount'] / (df['peak_engagement_hour'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply temporal feature engineering\n",
    "df_posts = create_temporal_features(df_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ccc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_viral_indicators(df):\n",
    "    \"\"\"\n",
    "    Calculate advanced viral potential indicators\n",
    "    \"\"\"\n",
    "    # Engagement ratios\n",
    "    df['like_share_ratio'] = df['diggCount'] / (df['shareCount'] + 1)\n",
    "    df['comment_like_ratio'] = df['comment_count'] / (df['diggCount'] + 1)\n",
    "    df['view_engagement_ratio'] = (df['diggCount'] + df['shareCount'] + df['comment_count']) / (df['playCount'] + 1)\n",
    "    \n",
    "    # Author influence factors\n",
    "    df['author_engagement_ratio'] = df['diggCount'] / (df['author_fans'] + 1)\n",
    "    df['follower_amplification'] = df['shareCount'] * np.log1p(df['author_fans'])\n",
    "    \n",
    "    # Community engagement diversity\n",
    "    df['commenter_diversity'] = df['unique_commenters'] / (df['comment_count'] + 1)\n",
    "    df['comment_engagement'] = df['total_comment_likes'] / (df['comment_count'] + 1)\n",
    "    \n",
    "    # Viral potential score (composite metric)\n",
    "    # Normalize features for scoring\n",
    "    engagement_features = ['view_engagement_ratio', 'comment_velocity', 'commenter_diversity', 'relative_engagement']\n",
    "    \n",
    "    for feature in engagement_features:\n",
    "        df[f'{feature}_normalized'] = (df[feature] - df[feature].mean()) / (df[feature].std() + 1e-8)\n",
    "    \n",
    "    df['viral_potential_score'] = (\n",
    "        df['view_engagement_ratio_normalized'] * 0.3 +\n",
    "        df['comment_velocity_normalized'] * 0.25 +\n",
    "        df['commenter_diversity_normalized'] * 0.25 +\n",
    "        df['relative_engagement_normalized'] * 0.2\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate viral indicators\n",
    "df_posts = calculate_viral_indicators(df_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "class TikTokTrendEnsemble:\n",
    "    def __init__(self):\n",
    "        # Base models optimized for social media analysis\n",
    "        self.base_models = {\n",
    "            'rf': RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=5,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'xgb': XGBClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                random_state=42,\n",
    "                eval_metric='logloss'\n",
    "            ),\n",
    "            'lgb': lgb.LGBMClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            ),\n",
    "            'lr': LogisticRegression(\n",
    "                C=1.0,\n",
    "                max_iter=1000,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'svm': SVC(\n",
    "                kernel='rbf',\n",
    "                C=1.0,\n",
    "                probability=True,\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Meta-learning ensemble\n",
    "        self.ensemble = VotingClassifier(\n",
    "            estimators=list(self.base_models.items()),\n",
    "            voting='soft'\n",
    "        )\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"\n",
    "        Prepare feature matrix for training\n",
    "        \"\"\"\n",
    "        # Select numerical features\n",
    "        feature_cols = []\n",
    "        \n",
    "        # Engagement features\n",
    "        engagement_features = [\n",
    "            'diggCount', 'shareCount', 'playCount', 'collectCount', 'comment_count',\n",
    "            'like_share_ratio', 'comment_like_ratio', 'view_engagement_ratio',\n",
    "            'engagement_velocity', 'comment_velocity', 'viral_potential_score'\n",
    "        ]\n",
    "        feature_cols.extend(engagement_features)\n",
    "        \n",
    "        # Temporal features\n",
    "        temporal_features = [\n",
    "            'hour_of_day', 'day_of_week', 'is_weekend',\n",
    "            'time_to_first_comment_hours', 'comment_time_span_hours'\n",
    "        ]\n",
    "        feature_cols.extend(temporal_features)\n",
    "        \n",
    "        # Sentiment features\n",
    "        sentiment_features = []\n",
    "        text_types = ['text', 'transcribed_text', 'all_comments']\n",
    "        for text_type in text_types:\n",
    "            sentiment_features.extend([\n",
    "                f'{text_type}_vader_compound',\n",
    "                f'{text_type}_vader_pos',\n",
    "                f'{text_type}_emotion_score'\n",
    "            ])\n",
    "        feature_cols.extend(sentiment_features)\n",
    "        \n",
    "        # Topic and hashtag features\n",
    "        feature_cols.extend(['topic_probability', 'has_trending_hashtag'])\n",
    "        \n",
    "        # Author features\n",
    "        feature_cols.extend(['author_fans', 'video_duration'])\n",
    "        \n",
    "        # Fill missing values and return feature matrix\n",
    "        feature_matrix = df[feature_cols].fillna(0)\n",
    "        return feature_matrix, feature_cols\n",
    "    \n",
    "    def train(self, df, target_col):\n",
    "        \"\"\"\n",
    "        Train ensemble model\n",
    "        \"\"\"\n",
    "        X, feature_names = self.prepare_features(df)\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Cross-validation evaluation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        print(\"Training individual models...\")\n",
    "        for name, model in self.base_models.items():\n",
    "            scores = cross_val_score(model, X, y, cv=cv, scoring='f1_weighted')\n",
    "            print(f\"{name}: F1 = {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "        \n",
    "        # Train ensemble\n",
    "        print(\"Training ensemble...\")\n",
    "        self.ensemble.fit(X, y)\n",
    "        ensemble_scores = cross_val_score(self.ensemble, X, y, cv=cv, scoring='f1_weighted')\n",
    "        print(f\"Ensemble: F1 = {ensemble_scores.mean():.4f} (+/- {ensemble_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        self.feature_names = feature_names\n",
    "        return self.ensemble\n",
    "\n",
    "# Create target variable (example: high engagement threshold)\n",
    "engagement_threshold = df_posts['viral_potential_score'].quantile(0.8)\n",
    "df_posts['is_trending'] = (df_posts['viral_potential_score'] >= engagement_threshold).astype(int)\n",
    "\n",
    "# Train ensemble model\n",
    "trend_ensemble = TikTokTrendEnsemble()\n",
    "trained_model = trend_ensemble.train(df_posts, 'is_trending')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7945f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class TikTokSequenceLSTM:\n",
    "    def __init__(self, sequence_length=24, lstm_units=128):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.lstm_units = lstm_units\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.model = None\n",
    "    \n",
    "    def create_sequences(self, df):\n",
    "        \"\"\"\n",
    "        Create time-based sequences for LSTM training\n",
    "        \"\"\"\n",
    "        # Sort by timestamp\n",
    "        df_sorted = df.sort_values('post_datetime')\n",
    "        \n",
    "        # Select time-series features\n",
    "        ts_features = [\n",
    "            'diggCount', 'shareCount', 'playCount', 'comment_count',\n",
    "            'engagement_velocity', 'viral_potential_score', 'hour_of_day'\n",
    "        ]\n",
    "        \n",
    "        # Normalize features\n",
    "        feature_data = self.scaler.fit_transform(df_sorted[ts_features])\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(self.sequence_length, len(feature_data)):\n",
    "            X.append(feature_data[i-self.sequence_length:i])\n",
    "            y.append(df_sorted.iloc[i]['is_trending'])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Build LSTM model architecture\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(self.lstm_units, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(0.2),\n",
    "            LSTM(self.lstm_units//2, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, df, epochs=50, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train LSTM model\n",
    "        \"\"\"\n",
    "        X, y = self.create_sequences(df)\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            print(\"Not enough data for sequence creation\")\n",
    "            return None\n",
    "        \n",
    "        # Split data\n",
    "        split_idx = int(0.8 * len(X))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        # Build and train model\n",
    "        self.model = self.build_model((X.shape[1], X.shape[2]))\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "# Train LSTM model\n",
    "if len(df_posts) >= 100:  # Ensure sufficient data\n",
    "    lstm_model = TikTokSequenceLSTM()\n",
    "    lstm_history = lstm_model.train(df_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d442246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    \"\"\"Extract hashtags from text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    hashtags = re.findall(r'#\\w+', text.lower())\n",
    "    return [tag.replace('#', '') for tag in hashtags]\n",
    "\n",
    "# Extract hashtags\n",
    "df_posts['hashtag_list'] = df_posts['text'].apply(extract_hashtags)\n",
    "df_posts['hashtag_text'] = df_posts['hashtag_list'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67297938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hashtags\n",
    "df_posts['hashtag_list'] = df_posts['text'].apply(extract_hashtags)\n",
    "df_posts['hashtag_text'] = df_posts['hashtag_list'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff473e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Show top 10 trending posts\n",
    "top_trends = df_posts[df_posts['is_trending']==True]\n",
    "print(top_trends[['viral_potential_score', 'primary_topic', 'hashtag_list']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Show top 10 trending posts\n",
    "top_trends = df_posts.sort_values('viral_potential_score', ascending=False).head(50)\n",
    "print(top_trends[['viral_potential_score', 'primary_topic', 'hashtag_list']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d15134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "all_hashtags = list(itertools.chain.from_iterable(top_trends['hashtag_list']))\n",
    "hashtag_counts = Counter(all_hashtags)\n",
    "top_hashtags = hashtag_counts.most_common(30)\n",
    "\n",
    "top_hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you used BERTopic\n",
    "topic_model.get_topic_info()  # Shows topic numbers and their top words\n",
    "\n",
    "# To get a human-readable label for a topic:\n",
    "for topic_num in trending_posts['primary_topic'].unique():\n",
    "    print(f\"Topic {topic_num}: {topic_model.get_topic(topic_num)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c04b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='primary_topic', data=top_trends)\n",
    "plt.title('Primary Topics of Trending Posts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def comprehensive_model_evaluation(model, X_test, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of trend detection model\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Classification metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # ROC AUC\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"\\nROC AUC Score: {auc_score:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance (for tree-based models)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(data=feature_imp.head(20), x='importance', y='feature')\n",
    "        plt.title('Top 20 Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'auc': auc_score,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "X_features, feature_names = trend_ensemble.prepare_features(df_posts)\n",
    "train_size = int(0.8 * len(X_features))\n",
    "X_test = X_features[train_size:]\n",
    "y_test = df_posts['is_trending'].iloc[train_size:]\n",
    "\n",
    "evaluation_results = comprehensive_model_evaluation(\n",
    "    trained_model, X_test, y_test, feature_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ca5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    \"\"\"Extract hashtags from text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    hashtags = re.findall(r'#\\w+', text.lower())\n",
    "    return [tag.replace('#', '') for tag in hashtags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d32044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hashtags\n",
    "df['hashtag_list'] = df['text'].apply(extract_hashtags)\n",
    "df['hashtag_text'] = df['hashtag_list'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36caeea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ec650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def create_tfidf_features(text_series, max_features=5000, ngram_range=(1,2)):\n",
    "    \"\"\"\n",
    "    Create TF-IDF features for text data\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        strip_accents='unicode',\n",
    "        lowercase=True\n",
    "    )\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_series.fillna(''))\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    return tfidf_matrix, vectorizer, feature_names\n",
    "\n",
    "# Create TF-IDF features for different text types\n",
    "transcript_tfidf, transcript_vectorizer, transcript_features = create_tfidf_features(df['transcribed_text'])\n",
    "comments_tfidf, comments_vectorizer, comments_features = create_tfidf_features(df['all_comments_text'])\n",
    "captions_tfidf, captions_vectorizer, captions_features = create_tfidf_features(df['text'])\n",
    "hashtag_tfidf, hashtag_vectorizer, hashtag_features = create_tfidf_features(df['hashtag_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "\n",
    "def create_word2vec_embeddings(text_series, vector_size=100, window=5, min_count=2):\n",
    "    \"\"\"\n",
    "    Create Word2Vec embeddings\n",
    "    \"\"\"\n",
    "    # Prepare sentences for Word2Vec\n",
    "    sentences = [simple_preprocess(text) for text in text_series.fillna('') if text.strip()]\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=4,\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def text_to_vector(text, model, vector_size=100):\n",
    "    \"\"\"\n",
    "    Convert text to average word vector\n",
    "    \"\"\"\n",
    "    words = simple_preprocess(text)\n",
    "    word_vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word])\n",
    "    \n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Create Word2Vec embeddings\n",
    "combined_text = pd.concat([\n",
    "    df['transcribed_text'], \n",
    "    df['comment'], \n",
    "    df['text']\n",
    "]).dropna()\n",
    "\n",
    "word2vec_model = create_word2vec_embeddings(combined_text)\n",
    "\n",
    "# Generate document vectors\n",
    "df['transcript_w2v'] = df['transcribed_text'].apply(\n",
    "    lambda x: text_to_vector(x, word2vec_model) if pd.notna(x) else np.zeros(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "def create_bert_embeddings(texts, model_name='bert-base-uncased', max_length=512):\n",
    "    \"\"\"\n",
    "    Create BERT embeddings for text data\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            if pd.isna(text) or text.strip() == '':\n",
    "                embeddings.append(np.zeros(768))  # BERT base has 768 dimensions\n",
    "                continue\n",
    "                \n",
    "            # Tokenize and encode\n",
    "            encoded = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Get BERT embeddings\n",
    "            outputs = model(**encoded)\n",
    "            \n",
    "            # Use [CLS] token embedding as document representation\n",
    "            cls_embedding = outputs.last_hidden_state[0][0].numpy()\n",
    "            embeddings.append(cls_embedding)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Create BERT embeddings (use smaller batches to manage memory)\n",
    "batch_size = 32\n",
    "bert_embeddings = []\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['transcribed_text'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = create_bert_embeddings(batch_texts)\n",
    "    bert_embeddings.extend(batch_embeddings)\n",
    "\n",
    "df['bert_embeddings'] = list(bert_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skincare-analysis-pFfgqJ8c-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
